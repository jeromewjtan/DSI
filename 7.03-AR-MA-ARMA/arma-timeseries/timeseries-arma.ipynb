{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Time Series Properties, Autoregressive, and Moving Average Models\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Justin Pounders_\n",
    "\n",
    "---\n",
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Describe and calculate a time series decomposition.\n",
    "- Calculate and plot ACF and PACF\n",
    "- Create autoregressive and moving average models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-the-unemployment-data\"></a>\n",
    "<a id=\"load-the-unemployment-data\"></a>\n",
    "## Load the Unemployment Data\n",
    "---\n",
    "\n",
    "This is historical quarterly data regarding unemployment rates in the United States. First, perform any necessary cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/seasonally-adjusted-quarterly-us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['year_quarter', 'unemployment_rate']\n",
    "data['unemployment_rate'] = data['unemployment_rate'].map(lambda x: float(str(x).replace('%','')))\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-a-datetime-index\"></a>\n",
    "<a id=\"create-a-datetime-index\"></a>\n",
    "## Create a Datetime Index\n",
    "---\n",
    "\n",
    "This is quarterly data, so converting to datetime is a bit complicated. `.dt.to_period('Q')` will help us represent the string as a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visually-examine-the-unemployment-rate\"></a>\n",
    "<a id=\"visually-examine-the-unemployment-rate\"></a>\n",
    "## Visually Examine the Unemployment Rate\n",
    "---\n",
    "\n",
    "Plot the unemployment rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemployment_rate'].plot(lw=2.5, figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ticks = data.index.to_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"autocorrelation-and-the-autocorrelation-function-acf\"></a>\n",
    "<a id=\"autocorrelation-and-the-autocorrelation-function-acf\"></a>\n",
    "##  Autocorrelation and the Autocorrelation Function (ACF)\n",
    "---\n",
    "\n",
    "In previous weeks, our analyses have been concerned with the correlation between two or more variables (height and weight, education and salary, etc.). In time series data, autocorrelation is a measure of _how correlated a variable is with itself_.\n",
    "\n",
    "Specifically, autocorrelation measures how closely earlier values relate to values that occur later in time.\n",
    "\n",
    "Examples of autocorrelation are:\n",
    "\n",
    "    In stock market data, the stock price at one point is correlated with the stock \n",
    "    price of the point that's directly prior in time. \n",
    "    \n",
    "    In sales data, sales on a Saturday are likely correlated with \n",
    "    sales on the next Saturday and the previous Saturday, as well as other days to more\n",
    "    or less of an extent.\n",
    "\n",
    "Below is the formula for the autocorrelation function (ACF):\n",
    "\n",
    "$\\text{Given measurements } x_1, x_2, x_3 ... x_n \\text{ at time points } t_1, t_2, t_3 ... t_n:$\n",
    "\n",
    "### $$lag_k\\;acf() = \\frac{\\sum_{t=k+1}^{n}\\left(\\;x_t - \\bar{x}\\;\\right)\\left(\\;x_{t-k} - \\bar{x}\\;\\right)}{\\sum_{t=1}^n\\left(\\;x_t - \\bar{x}\\;\\right)^2}$$\n",
    "\n",
    "Compare this to the formula for correlation:\n",
    "\n",
    "$\\text{Given measurements } x_1, x_2, x_3 ... x_n \\text{ and measurements } y_1, y_2, y_3 ... y_n:$\n",
    "\n",
    "### $$r_{xy} = \\frac{\\sum_{i=1}^{n}\\left(\\;x_i - \\bar{x}\\;\\right)\\left(\\;y_{i} - \\bar{y}\\;\\right)}{\\sqrt{\\left(\\sum_{i=1}^{n}\\left(\\;x_i - \\bar{x}\\;\\right)^2\\sum_{i=1}^n\\left(\\;y_i - \\bar{y}\\;\\right)^2\\right)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"computing-autocorrelation\"></a>\n",
    "<a id=\"computing-autocorrelation\"></a>\n",
    "### Computing Autocorrelation\n",
    "\n",
    "To compute autocorrelation, we fix a lag, _k_, which is the delta between the given point and the prior point used to compute the correlation.\n",
    "\n",
    "With a _k_ value of 1, we'd compute how correlated a value is with the value that's immediately prior. With a _k_ value of 10, we'd compute how correlated a variable is with the value that's 10 time points prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"autocorrelation-using-statsmodels\"></a>\n",
    "<a id=\"autocorrelation-using-statsmodels\"></a>\n",
    "### Autocorrelation Using Statsmodels\n",
    "\n",
    "Statsmodels comes with some convenient packages for calculating and plotting autocorrelation. Load up these two functions and try them out:\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C1) **Check:** \n",
    "\n",
    "1. What does autocorrelation tell you?\n",
    "2. What is a shortcoming of autocorrelation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"partial-autocorrelation-and-the-partial-autocorrelation-function-pacf\"></a>\n",
    "<a id=\"partial-autocorrelation-and-the-partial-autocorrelation-function-pacf\"></a>\n",
    "## Partial Autocorrelation and the Partial Autocorrelation Function (PACF)\n",
    "---\n",
    "\n",
    "Another important chart for diagnosing your time series is the partial autocorrelation chart (PACF). This is similar to autocorrelation, but, instead of being just the correlation at increasing lags, it is the correlation at a given lag, _controlling for the effect of previous lags._\n",
    "\n",
    "Load up the sister functions for partial autocorrelation from Statsmodels and test them out on the differenced time series.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Time Series Decomposition\n",
    "\n",
    "$$ Y_t = T_t + S_t + C_t + \\varepsilon_t$$\n",
    "\n",
    "- $Y_t = $ observed value at time $t$\n",
    "- $T_t = $ trend component, *long-run behavior*\n",
    "- $S_t = $ seasonality component, *periodic fluctuations*\n",
    "- $C_t = $ cyclical component, *non-periodic fluctuations*\n",
    "- $\\varepsilon_t = $ noise component, *we would like this to be stationary*\n",
    "\n",
    "> Decomposition above is additive; can also be multiplicative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several algorithms for performing this decomposition\n",
    "\n",
    "- Classical decomp., [https://www.otexts.org/fpp/6/3](https://www.otexts.org/fpp/6/3)\n",
    "- X-12-ARIMA, [https://www.otexts.org/fpp/6/4](https://www.otexts.org/fpp/6/4)\n",
    "- STL, [https://www.otexts.org/fpp/6/5](https://www.otexts.org/fpp/6/5)\n",
    "\n",
    "See also `seasonal_decompose` in `statsmodel.tsa.seasonal`.  \n",
    "\n",
    "> Nice tutorial here: [http://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/](http://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "n = 201\n",
    "np.random.seed(42)\n",
    "t = np.linspace(0, 500, n)\n",
    "signal = (t/250)**2 + np.sin(t/10) + np.random.normal(scale=0.15, size=n)\n",
    "\n",
    "d0 = datetime.datetime.now()\n",
    "dt1 = datetime.timedelta(1)\n",
    "signal = pd.Series(signal, index=[d0+dt1*i for i in t])\n",
    "\n",
    "signal.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C2) **Is the above *stationary***?\n",
    "\n",
    "**Questions:** \n",
    "- *What is the long-term behavior of my series?*\n",
    "- *How does my time series fluctuate?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(signal)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... definitely some seasonality still left in the residual. From the `statsmodels` docs:\n",
    "\n",
    "> \"This is a naive decomposition. More sophisticated methods should be preferred.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "tsindex = data.index.to_timestamp()\n",
    "data_ts = data.copy()\n",
    "data_ts.index = tsindex\n",
    "\n",
    "result = seasonal_decompose(data_ts.unemployment_rate)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.resid[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C2) **Check**\n",
    "\n",
    "Which of the three trends addresses the following questions?\n",
    "1. What is the long-term behavior of my series?\n",
    "2. What is the effect of time-of-day on my series?\n",
    "3. What is the effect of larger, unseen fluctuations on my series?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"differencing-a-timeseries-and-stationarity\"></a>\n",
    "<a id=\"differencing-a-timeseries-and-stationarity\"></a>\n",
    "## \"Differencing\" a Time Series and Stationarity\n",
    "---\n",
    "\n",
    "If a time series is stationary, the mean, variance, and autocorrelation (covered in the next section) will be constant over time. \n",
    "\n",
    "> Forecasting methods typically assume the time series you are forecasting on to be stationary — or at least approximately stationary.\n",
    "\n",
    "To stationarize (maybe not a word...) we could possibly take two approaches:\n",
    "\n",
    "1. Pull the \"residual\" component from the decomposition and check if it is stationary, or\n",
    "2. Take a \"diff\" one or more times.\n",
    "\n",
    "The most common way to make a time series stationary is through \"differencing.\" This procedure converts a time series into the difference between values.\n",
    "\n",
    "<a id=\"-delta-yt--yt---yt--\"></a>\n",
    "### $$ \\Delta y_t = y_t - y_{t-1} $$\n",
    "\n",
    "This removes trends in the time series and ensures that the mean across time is zero. In most cases, this only requires a single difference, although, in some cases, a second difference (or third, etc.) will be necessary to remove trends.\n",
    "\n",
    "**Difference the unemployment rate and plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How do you really know if your timeseries is stationary?  You can formulate stationarity as a hypothesis and then test the hypothesis!  An example of this approach is the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test).  We'll look at this in the afternoon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"autoregressive-ar-models\"></a>\n",
    "<a id=\"autoregressive-ar-models\"></a>\n",
    "## Autoregressive (AR) Models\n",
    "\n",
    "---\n",
    "\n",
    "Autoregressive (AR) models use data from previous time points to predict the next time point. These are essentially regression models in which the predictors are previous time points of the outcome.\n",
    "\n",
    "Typically, AR models are denoted as `AR(p)`, where _p_ indicates the number of previous time points to incorporate. `AR(1)` is the most common.\n",
    "\n",
    "In an autoregressive model, we learn regression coefficients on the features that are the previous _p_ values.\n",
    "\n",
    "### $$y_i = c + \\beta_1  y_{i-1} + \\beta_2  y_{i-2}\\ +\\ ...\\ +\\ \\beta_p  y_{i-p}\\ +\\ \\epsilon \\\\\n",
    "y_i =\\sum_{j=1}^p \\beta_j y_{i-j} + \\epsilon$$\n",
    "\n",
    "We can build autoregressive models using the `ARMA` class from Statsmodels. (Alternatively, there is a newer Python package, PyFlux, that also looks promising for time series modeling.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "import pyflux as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"In-sample\" predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = None\n",
    "full_pred = None\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], data.unemployment_rate[1:], lw=2, color='darkred', label='Data')\n",
    "ax.plot(date_ticks[1:], full_pred, lw=2, color='grey', ls='dashed', label='Fit')\n",
    "plt.legend()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed',label='Data')\n",
    "ax.plot(date_ticks[1:], predictions, lw=2, color='darkred',label='Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Out-of-sample\" predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_oos = None\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed', label='Data')\n",
    "ax.plot(date_ticks[1:], predictions, lw=2, color='darkred', label='In-sample predictions')\n",
    "ax.plot(date_ticks[101:], predictions_oos, lw=2, color='blue', label='Out-of-sample predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C3) **Check**\n",
    "\n",
    "What's the difference between in-sample and out-of-sample prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"moving-average-ma-models\"></a>\n",
    "<a id=\"moving-average-ma-models\"></a>\n",
    "## Moving Average (MA) Models\n",
    "---\n",
    "\n",
    "**Moving average models** take previous _error terms_ as inputs. They predict the next value based on deviations from previous predictions. _This can be useful for modeling a sudden occurrence — e.g., something going out of stock affecting sales or a sudden rise in popularity._\n",
    "\n",
    "As with autoregressive models, we have an order term, _q_, and we refer to our model as `MA(q)`. This moving average model is dependent on the last _q_ errors. If we have a time series of sales per week, $y_i$, we can regress each $y_i$ on the last _q_ error terms.\n",
    "\n",
    "### $$y_t = \\epsilon_t + \\beta_{1} \\epsilon_{t-1} + ... \\beta_{n} \\epsilon_{t-n} \\\\\n",
    "y_t = \\sum_{i=1}^n \\beta_i \\epsilon_{t-i} + \\epsilon_t$$\n",
    "\n",
    "Sometimes, the mean of the time series is included in the equation:\n",
    "\n",
    "### $$ y_t = \\mu + \\sum_{i=1}^n \\beta_i \\epsilon_{t-i} + \\epsilon_t $$\n",
    "\n",
    "Moving average models are not as trivial to fit as autoregressive models because the error terms are unobserved. There are a variety of different ways you can estimate the parameters, some of which are covered in [this paper](https://www.it.uu.se/research/publications/reports/2006-022/2006-022-nc.pdf).\n",
    "\n",
    "With the simpler fitting procedures, a model is iteratively fit, then errors are computed and refit over and over again until the parameters on the errors converge.\n",
    "\n",
    "MA includes the mean of the time series. The behavior of the model is therefore characterized by random jumps around the mean value.\n",
    "\n",
    "In an `MA(1)` model, one coefficient on the error of our previous prediction affects our estimate for the next value in the time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed',label='Data')\n",
    "ax.plot(date_ticks[1:], ma1.fittedvalues, lw=2, color='darkred',label='Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"arma-and-arima-models\"></a>\n",
    "<a id=\"arma-and-arima-models\"></a>\n",
    "## ARMA and ARIMA Models\n",
    "---\n",
    "\n",
    "**ARMA** models combine autoregressive models and moving average models, parameterizing the behavior of models with `p` and `q` terms that correspond to `AR(p)` models and `MA(q)` models.\n",
    "\n",
    "Autoregressive models slowly incorporate changes in preferences, tastes, and patterns. Moving average models base their predictions not on the prior value but instead on the prior error, allowing us to correct sudden changes based on random events — supply, popularity spikes, etc.\n",
    "\n",
    "**ARIMA** is just like the `ARMA(p, q)` model, but, rather than predicting the value of the series, it predicts the _differenced_ series or changes in the series. The order of differencing is set by a _d_ term — as in `ARIMA(p, d, q)` — or alternatively, you can just fit an `ARMA(p, q)` model on a differenced time series.\n",
    "\n",
    "Recall the Pandas `diff()` function. This computes the difference between two consecutive values. In an ARIMA model, we attempt to predict this difference instead of the actual values.\n",
    "\n",
    "### $$y_t - y_{(t-1)} = ARMA(p, q)$$\n",
    "\n",
    "In modeling, we assume time series to be \"stationary.\" This handles the stationarity assumption: Instead of detrending or differencing manually, models do this via differencing terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed',label='Data')\n",
    "ax.plot(date_ticks[1:], ar1ma1.fittedvalues, lw=2, color='darkred',label='Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pred = data.unemployment_rate.values[0]+np.cumsum(ar1ma1.fittedvalues)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], full_pred, lw=2, color='grey', ls='dashed')\n",
    "ax.plot(date_ticks[1:], data.unemployment_rate[1:], lw=2, color='darkred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-to-choose-the-right-p-and-q-parameters\"></a>\n",
    "<a id=\"how-to-choose-the-right-p-and-q-parameters\"></a>\n",
    "## How to Choose the Right `p` and `q` Parameters\n",
    "---\n",
    "\n",
    "In general, it's never a bad idea to choose your parameters based on hold-out testing; that is to say, checking the performance of your model on future time points based on different choices of `p` and `q` for an ARIMA model.\n",
    "\n",
    "However, you can get a sense for which parameters will work best based on autocorrelation and partial autocorrelation charts.\n",
    "\n",
    "[This site](https://people.duke.edu/~rnau/411arim3.htm) offers a detailed overview of how to use ACF and PACF to determine your parameters.\n",
    "\n",
    "Below are some basic general guidelines. Remember that these rules apply to the ACF and PACF plots of differenced time series rather than the original time series (the exception being if your time series is stationary and does not require differencing):\n",
    "\n",
    "1. If the PACF has a sharp cut-off and the lag-1 ACF value is positive, choose an `AR(x)` term where `x` is the lag in the PACF after the cut-off.\n",
    "2. If the ACF has a sharp cut-off and the lag-1 ACF value is negative, choose an `MA(x)` term where `x` is the lag in the ACF after the cut-off.\n",
    "3. If both the ACF and PACF show a gradual decay, an ARMA model is likely appropriate as opposed to AR or MA alone.\n",
    "\n",
    "Context 1 above corresponds to time series that are \"underdifferenced,\" as indicated by a positive autocorrelation at lat 1. Likewise, Context 2 is \"overdifferenced,\" as indicated by a negative autocorrelation.\n",
    "\n",
    "In general, you should try to choose an AR or MA model as opposed to an ARMA model. The AR and MA terms can work against each other in the model and create an overly complex representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
