{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Linear Regression\n",
    "\n",
    "_Authors: Matt Brems (DC), Marc Harper (LA)_\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives \n",
    "- Understand modeling, inference from models, and prediction\n",
    "- Understand the framework of regression modeling\n",
    "- Understand simple linear regression and linear relationships between variables\n",
    "- Learn how to find the best slope and intercept for simple linear regression\n",
    "- Be able to interpret the results of simple linear regression\n",
    "- Understand the assumptions that simple linear regression makes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Naive/Baseline Prediction](#naivebaseline-prediction)\n",
    "- [Finding the \"best\" slope and intercept](#finding-the-best-slope-and-intercept)\n",
    "- [Interpretation of SLR](#interpretation-of-slr)\n",
    "- [Assumptions of SLR](#assumptions-of-slr)\n",
    "- [Final notes on linear regression](#final-notes-on-linear-regression)\n",
    "- [Categorical predictors](#categorical-predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"naivebaseline-prediction\"></a>\n",
    "## Naive/Baseline Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best prediction we can make for any data point if we can only use a single value?\n",
    "\n",
    "How can we measure the success of our prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lets-generate-some-data\"></a>\n",
    "### Let's generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an x and a y that is \n",
    "# 50 plus 2 times x with some random noise around that\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know very little about our data and can only use a single value, we can still predict the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot the data\n",
    "# create a figure\n",
    "\n",
    "\n",
    "# get current axis of that figure\n",
    "\n",
    "\n",
    "# plot a scatter plot on it with our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add that prediction to our DataFrame to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"finding-the-best-slope-and-intercept\"></a>\n",
    "## Finding the \"best\" slope and intercept\n",
    "\n",
    "---\n",
    "\n",
    "**1. If we want to find the \"best\" slope an intercept, we will need a criterion that defines what it means to be the \"best\".**\n",
    "\n",
    "**2. We need to be able to numerically optimize (maximize or minimize) this criterion.**\n",
    "\n",
    "\n",
    "<a id=\"residuals\"></a>\n",
    "### Residuals\n",
    "\n",
    "The difference between the true value of $Y$ and the predicted value of $Y$ (denoted $\\hat{Y}$, \"Y-hat\") is known as a \"residual.\" We calculate a residual as $Y_i - \\hat{Y}_i$ and denote each residual as $\\varepsilon_i$.\n",
    "\n",
    "**Note that the residual $\\varepsilon_i$ is the same as the error in our modeled linear relationship!**\n",
    "\n",
    "For every value of $i$ from 1 to $n$, we:\n",
    "- have an observed $X_i$.\n",
    "- have an observed $Y_i$.\n",
    "- can calculate a predicted value $\\hat{Y}_i$, which is equal to $f(X_i)$, where $f$ is our model.\n",
    "- can calculate a residual $\\varepsilon_i$, which is equal to $Y_i - \\hat{Y}_i$.\n",
    "\n",
    "<a id=\"residual-sum-of-squares-rss\"></a>\n",
    "### Residual sum of squares (RSS)\n",
    "\n",
    "The most common way to define the \"best fit line\" is one that forces our residuals (a.k.a. errors) will be as close to zero as possible.\n",
    "\n",
    "In fact, we seek to minimize the *sum of squared residuals* or [residual sum of squares (RSS)](https://en.wikipedia.org/wiki/Residual_sum_of_squares).\n",
    "\n",
    "### $$RSS = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 $$\n",
    "\n",
    "This is also known as a **loss function**, where the \"loss\" here is the sum of squared residuals.\n",
    "\n",
    "---\n",
    "\n",
    "**Review: what is the difference between a parameter and a statistic?**\n",
    "\n",
    "Below are two equations, one using parameters and one using statistics. What is the difference?\n",
    "\n",
    "### $$ Y = \\beta_0 + \\beta_1 X \\\\\n",
    "\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our naive prediction? Let's see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate SSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"minimizing-the-mse\"></a>\n",
    "### Minimizing the MSE\n",
    "\n",
    "For simple linear regression [we can derive the intercept and slope that minimize the RSS algebraically](https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line). \n",
    "\n",
    "Given variables:\n",
    "- $\\bar{y}$ : the sample mean of observed values $Y$\n",
    "- $\\bar{x}$ : the sample mean of observed values $X$\n",
    "- $s_Y$ : the sample standard deviation of observed values $Y$\n",
    "- $s_X$ : the sample standard deviation of observed values $X$\n",
    "- $r_{XY}$ : the sample Pearson correlation coefficient between observed $X$ and $Y$\n",
    "\n",
    "The equations for the intercept and slope that minimize the RSS is:\n",
    "\n",
    "### $$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\n",
    "\\hat{\\beta}_1 = r_{XY} \\frac{s_Y}{s_X}$$\n",
    "\n",
    "Our simple linear regression is an **estimator of the expected value (mean) of $Y$**.\n",
    "\n",
    "> **Note:** Estimators are mathematical functions or formulas that yield an *estimate*. For example, an estimator might be the sample mean whereas the estimate is the value of the sample mean from our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"calculate-beta-coefficients-by-hand\"></a>\n",
    "### Calculate beta coefficients (a.k.a. \"fit\" model) by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_bar = \n",
    "x_bar = \n",
    "std_y = \n",
    "std_x = \n",
    "r_xy = \n",
    "\n",
    "beta_1 = \n",
    "beta_0 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with model\n",
    "df['Linear_Yhat'] = beta_0 + beta_1 * df['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate MSE\n",
    "np.square(df['Y'] - df['Linear_Yhat']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does our new MSE compare to our old MSE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now plot our line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also plot our residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "# get the axis\n",
    "ax = fig.gca()\n",
    "\n",
    "# plot it\n",
    "ax.scatter(x=df['X'], y=df['Y'], c='k')\n",
    "ax.plot(df['X'], df['Linear_Yhat'], color='k');\n",
    "\n",
    "# iterate over predictions\n",
    "for _, row in df.iterrows():\n",
    "    plt.plot((row['X'], row['X']), (row['Y'], row['Linear_Yhat']), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"interpretation-of-slr\"></a>\n",
    "## Interpretation of SLR\n",
    "\n",
    "---\n",
    "\n",
    "Suppose I believe that commute time, my $Y$, is best modeled as a linear function of the number of Metro stops I take. Then, my model is:\n",
    "### $$ [commute] = \\beta_0 + \\beta_1 [Metro\\_stops] $$\n",
    "\n",
    "I want to estimate this model from our data. I gather data over a long period of time and put it into a Pandas dataframe with a column for commute time and a column for number of Metro stops. Then, my estimate of the true model is:\n",
    "\n",
    "### $$ \\hat{[commute]} = \\hat{\\beta}_0 + \\hat{\\beta}_1 [Metro\\_stops] $$\n",
    "\n",
    "Let's say that $\\hat{\\beta}_0$ is 5 and $\\hat{\\beta}_1$ is 3.\n",
    "- I interpret $\\hat{\\beta}_1$ by saying \"As the number of Metro stops increases by 1, I expect commute time to increase by three minutes.\"\n",
    "- I interpret $\\hat{\\beta}_0$ by saying \"If someone travels zero stops on the Metro, I expect commute time to be five minutes.\"\n",
    "\n",
    "The simple linear regression estimators described above have really nice properties:\n",
    "\n",
    "1. They are unbiased. (a.k.a. the statistics will on average equal the true parameter value)\n",
    "2. Among all unbiased estimators, the estimators above have the smallest possible variance.\n",
    "3. They are consistent. (a.k.a. as your sample size n goes to infinity, the statistic converges to the true parameter value)\n",
    "\n",
    "**However, these are just mathematical properties.** They do not take into account real-world situations. For example, suppose I want to use a SLR model to predict my commute time and I get the model:\n",
    "\n",
    "    [commute time] = 8 * [number of Metro stops] - 10\n",
    "\n",
    "In this case, my commute time is estimated to be negative if I go zero stops or one stop on the Metro. If I were to interpret my intercept, I might say \"If I travel zero Metro stops on my way to work, I expect to commute -10 minutes.\" Of course, this is nonsense; you can't have a negative commute time!\n",
    "\n",
    "**What this means:** \n",
    "\n",
    "When building a model, make sure your model makes sense! You are responsible for the interpretation of the model. All the computer will do is optimize. You may not do anything wrong when you fit a model (and you won't get an error!), but make a note that there are shortcomings to the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assumptions'></a>\n",
    "\n",
    "<a id=\"assumptions-of-slr\"></a>\n",
    "## Assumptions of SLR\n",
    "\n",
    "---\n",
    "\n",
    "There are some critical assumptions involved in SLR that you must be aware of:\n",
    "\n",
    "1. **Linearity:** Y and X must have an approximately linear relationship.\n",
    "2. **Independence:** Errors (residuals) $\\varepsilon_i$ and $\\varepsilon_j$ must be independent of one another for any $i \\neq j$.\n",
    "3. **Normality:** The errors (residuals) follow a Normal distribution with mean 0.\n",
    "4. **Equality of Variances** (Homoscedasticity of errors): The errors (residuals) should have a roughly consistent pattern, regardless of the value of X. (There should be no discernable relationship between X and the residuals.)\n",
    "\n",
    "The mnemonic **LINE** is a useful way to remember these four assumptions.\n",
    "\n",
    "Lengthier discussions of the four main assumptions and how one can evaluate or test for each of these [can be found here](http://people.duke.edu/~rnau/testing.htm).\n",
    "\n",
    "If all four assumptions are true, the following holds: \n",
    "\n",
    "### $$ Y_i \\sim N(\\beta_0 + \\beta_1 X_i, \\sigma) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "\n",
    "## Load the diamonds data\n",
    "\n",
    "---\n",
    "\n",
    "Over the course of this lesson we will be constructing a simple linear regression and then extend this to multiple linear regression. Included in the datasets folder is a dataset on diamond prices.\n",
    "\n",
    "**Load the data using pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diamonds_csv = './datasets/diamonds.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "\n",
    "    carat: the carat weight of diamonds\n",
    "    cut, color, clarity: (ignore)\n",
    "    depth, table, x, y, z: diamond measurements\n",
    "    price: the price of the diamond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='slr-to-mlr'></a>\n",
    "\n",
    "## From simple linear regression (SLR) to multiple linear regression (MLR)\n",
    "\n",
    "---\n",
    "\n",
    "The TL;DR of multiple linear regression:\n",
    "\n",
    "> Instead of using just one predictor to estimate a continuous target, we build a model with multiple predictor variables. You will be using MLR way more than SLR going forward.\n",
    "\n",
    "These variables will be represented as columns in a matrix (often a pandas dataframe).\n",
    "\n",
    "**Brainstorm some examples of real-world scenarios where multiple predictors would be beneficial. Can you think of cases where it might be detrimental?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assumptions'></a>\n",
    "\n",
    "## Assumptions of MLR\n",
    "\n",
    "---\n",
    "\n",
    "Like SLR, there are assumptions associated with MLR. Luckily, they're quite similar to the SLR assumptions.\n",
    "\n",
    "1. **Linearity:** $Y$ must have an approximately linear relationship with each independent $X_i$.\n",
    "2. **Independence:** Errors (residuals) $\\varepsilon_i$ and $\\varepsilon_j$ must be independent of one another for any $i \\ne j$.\n",
    "3. **Normality:** The errors (residuals) follow a Normal distribution with mean 0.\n",
    "4. **Equality of Variances**: The errors (residuals) should have a roughly consistent pattern, regardless of the value of the $X_i$ predictors. (There should be no discernable relationship between the $X$ predictors and the residuals.)\n",
    "5. **Independence of Predictors**: The independent variables $X_i$ and $X_j$ must be independent of one another for any $i \\ne j$.\n",
    "\n",
    "The mnemonic LINEI is a useful way to remember these five assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-mlr'></a>\n",
    "\n",
    "## Fitting a multiple linear regression\n",
    "\n",
    "---\n",
    "\n",
    "The formula for computing the $\\beta$ values in multiple regression is best done using linear algebra. We will cover the derivation, but for more depth  [these slides are a great resource](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf).\n",
    "\n",
    "$X$ is now a _matrix_ of predictors $x_1$ through $x_i$ (with each column a predictor), and $y$ is the target vector we are seeking to estimate. There is still only 1 *estimated* variable!\n",
    "\n",
    "### $$ \\hat{y} = X \\beta$$\n",
    "\n",
    "**Note:** $\\beta$ in the formula above is a *vector* of coefficients now, rather than a single value.\n",
    "\n",
    "In different notation we could write $\\hat{y}$ calculated with:\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n",
    "\n",
    "---\n",
    "\n",
    "<a id='mlr-beta-derivation'></a>\n",
    "\n",
    "### Deriving the $\\beta$ coefficients\n",
    "\n",
    "$\\beta$ is solved with the linear algebra formula:\n",
    "\n",
    "### $$ \\beta = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "Where $X'$ is the transposed matrix of original matrix $X$ and $(X^TX)^-1$ is the inverted matrix of $X^TX$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation using true $y$ is:\n",
    "\n",
    "### $$ y = X \\beta + \\varepsilon $$\n",
    "\n",
    "Again, $\\varepsilon$ is our vector of errors, or residuals.\n",
    "\n",
    "We can equivalently formulate this as in terms of the residuals:\n",
    "\n",
    "### $$ \\varepsilon = X \\beta - y $$\n",
    "\n",
    "Our goal is to minimize the sum of squared residuals. The sum of squared residuals is equivalent to the dot product of the vector of residuals:\n",
    "\n",
    "### $$ \\sum_{i=1}^n \\varepsilon_i^2 = \n",
    "\\left[\\begin{array}{cc}\n",
    "\\varepsilon_1 \\cdots \\varepsilon_n\n",
    "\\end{array}\\right] \n",
    "\\left[\\begin{array}{cc}\n",
    "\\varepsilon_1 \\\\ \\cdots \\\\ \\varepsilon_n\n",
    "\\end{array}\\right] = \\varepsilon^T \\varepsilon\n",
    "$$\n",
    "\n",
    "Therefore we can write the sum of squared residuals as:\n",
    "\n",
    "### $$ \\varepsilon^T \\varepsilon = (X \\beta - y)^T (X \\beta - y) $$\n",
    "\n",
    "Which becomes:\n",
    "\n",
    "### $$ \\varepsilon^T \\varepsilon = y^Ty - y^TX\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta $$\n",
    "\n",
    "Now take the derivative with respect to $\\beta$:\n",
    "\n",
    "### $$ \\frac{\\partial \\varepsilon^T \\varepsilon}{\\partial \\beta} = \n",
    "-2X^Ty + 2X^TX\\beta$$\n",
    "\n",
    "We want to minimize the sum of squared errors, and so we set the derivative to zero and solve for the beta coefficient vector:\n",
    "\n",
    "### $$ 0 = -2X^Ty + 2X^TX\\beta \\\\\n",
    "X^TX\\beta = X^Ty \\\\\n",
    "\\beta = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-mlr'></a>\n",
    "\n",
    "### Code a MLR\n",
    "\n",
    "**First, we need to create the \"design matrix\" of our predictors.**\n",
    "\n",
    "The first column will be a column of all 1s (the intercept) and the other columns will be `carat`, `depth`, `table`, `x`, `y`, `z`.\n",
    "\n",
    "This is easiest to do with pandas: add a column for the intercept first, then extract the matrix using with `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for the beta coefficients\n",
    "\n",
    "We are still predicting `price`. Implement the linear algebra equation to solve for the beta coefficients. \n",
    "\n",
    "### $$ \\beta = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "The transpose of a matrix is calculated by appending `.T` to the matrix:\n",
    "\n",
    "    X.T\n",
    "\n",
    "Matrices multipled in the formula should be done with the \"dot product\":\n",
    "\n",
    "    np.dot(mat1, mat2)\n",
    "\n",
    "Inverting a matrix is done using:\n",
    "\n",
    "    np.linalg.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm that these betas are the same as the ones using `sklearn.linear_model.LinearRegression`**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression(fit_intercept=False)\n",
    "linreg.fit(X, price)\n",
    "\n",
    "print(linreg.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate predicted $\\hat{y}$ with your $X$ predictor matrix and $\\beta$ coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the RMSE of the multiple regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Multiple regression MSE:', np.sqrt(np.mean(np.square(price - yhat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "[Maximum likelihood estimation](https://onlinecourses.science.psu.edu/stat504/node/28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "##  `sklearn` & `statsmodels`\n",
    "\n",
    "---\n",
    "\n",
    "This lesson intends to introduce the modeling packages `sklearn` and `statsmodels` in the context of regression modeling. These are both powerful python packages with different strengths. \n",
    "\n",
    "In general:\n",
    "- **`sklearn`** is the *machine learning* package\n",
    "- **`statsmodels`** is the *statistics* package\n",
    "\n",
    "Though the terms have immense overlap, machine learning tends to be more prediction focused while statistics is more inference focused. \n",
    "\n",
    "**Remember: even with all the power provided by these modeling tools, it's never a replacement for good EDA!**\n",
    "\n",
    "---\n",
    "\n",
    "### A preface on modeling\n",
    "\n",
    "As we venture down the path of modeling, it can be difficult to determine which choices are \"correct\" or \"incorrect\".  A primary challenge is to understand how different models will perform in different circumstances and different types of data. It's essential to practice modeling on a variety of data.\n",
    "\n",
    "As a beginner it is essential to learn which metrics are important for evaluating your models and what they mean. The metrics we evaluate our models with inform our actions.  \n",
    "\n",
    "*Exploring datasets on your own with the skills and tools you learn in class is highly recommended!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='documentation'></a>\n",
    "\n",
    "## `sklearn` and `statsmodels` documentation\n",
    "\n",
    "---\n",
    "\n",
    "Get familiar with looking up things in the `sklearn` and `statsmodels` documentation. You are going to be doing a lot of it over the course of DSI and beyond.\n",
    "\n",
    "[The statsmodels documentation can be found here.](http://statsmodels.sourceforge.net/devel/) Many recommend using the bleeding-edge version of statsmodels. [For that you can reference the code on github.](https://github.com/statsmodels/statsmodels/)\n",
    "\n",
    "[The sklearn documentation can be found here.](http://scikit-learn.org/stable/documentation.html)\n",
    "\n",
    "The packages have fairly different approaches and syntax for constructing models. Below are examples for linear regression in each package:\n",
    "* [Linear regression in statsmodels](http://statsmodels.sourceforge.net/devel/examples/#regression)\n",
    "* [Linear regression in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "If you haven't yet, familliarize yourself with the format of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-background'></a>\n",
    "\n",
    "## Background: Scikit-learn / sklearn\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://avatars1.githubusercontent.com/u/25111?v=3&s=200\" style=\"float: left; margin: 0 25px;\"> Scikit-learn was founded in 2007 as a Google summer of code project by [David Cournapeau](https://github.com/cournape).  Later in 2007, Matthieu Brucher published his thesis on Scikit-learn.  Since then, the Scikit-learn project has taken on a worldwide team of owners. A great high level overview of the project can be found in a 2011 publication in [Journal of Machine Learning Research 12 (2011) 2825-2830](http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf).\n",
    "\n",
    "Skikit-learn is part of the Scipy family of \"kits\".  Explore some of the [other projects](https://scikits.appspot.com/scikits) in this family.\n",
    "<br clear=\"all\"><br>\n",
    "\n",
    "\n",
    "**Scikit-learn provides a wide variety of machine learning models, including:**\n",
    "\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Support Vector Machines\n",
    "- Classification And Regression Tree Models\n",
    "- Naive Bayes\n",
    "- Clustering Models (K-Means, Hierarchical, DBScan)\n",
    "\n",
    "**It also handles the construction of typical machine learning pipeline utilities for:**\n",
    "- Model evaluation\n",
    "- Model selection\n",
    "- Preprocessing\n",
    "- Natural Language Processing\n",
    "- Dimensionality Reduction\n",
    "\n",
    "**Scikit-learn comes with a ton of datasets that are cleaned and formatted to work with the models provided by their library:**\n",
    "- Boston Housing\n",
    "- Iris Flowers\n",
    "- Diabetes Diagnostics\n",
    "- Various sample images (for classification)\n",
    "  - Faces\n",
    "  - MNIST (handwriting examples)\n",
    "- Random data generators\n",
    "- Spam examples\n",
    "- Newsgroup classfication\n",
    "\n",
    "[Read more about Scikit-learn datasets](http://scikit-learn.org/stable/datasets/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sklearn` under the hood\n",
    "\n",
    "- **Numpy**:  The base for data structures and transformations. Input data is represented as numpy arrays, integrating seamlessly with other scientific Python libraries. Numpy’s viewbased memory model limits copies, even when binding with compiled code.  It also provides basic arithmetic and linear algebra operations.<br><br>\n",
    "\n",
    "- **Scipy**:  Efficient algorithms for linear algebra, sparse matrix representation, special functions and basic statistical functions.<br><br>\n",
    "\n",
    "- **Cython**:  A language for combining C with Python. Cython makes it easy to reach the performance of compiled languages with Python-like syntax and high-level operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-first-steps'></a>\n",
    "\n",
    "## First steps with `sklearn`: loading the data\n",
    "\n",
    "---\n",
    "\n",
    "We will fit a linear regression model on the data with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the data in pandas for convenience\n",
    "\n",
    "Our target is what we are predicting.  Sometimes this is called the **response variable**.\n",
    "\n",
    "The target and the data are what we use to train, or **fit** the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create dataframe of main data to use as predictors (later). AKA \"X\"\n",
    "df =\n",
    "\n",
    "# target vector (price)\n",
    "target =\n",
    "\n",
    "print(target.shape)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set is a matrix / dataframe with six variables. We have **6** predictors with **53940** rows/observations.\n",
    "\n",
    "Our target is a vector that represents a single variable (**price**), which has exactly the same number of observations as our training set: **53940**.\n",
    "\n",
    "> _Training (fit) and target datasets must always match in length!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index correspondence between target and predictors\n",
    "\n",
    "Row 0 of **`df`**, our training data, is:\n",
    "\n",
    "```\n",
    "[0.23\t61.5\t55.0\t3.95\t3.98\t2.43]\n",
    "```\n",
    "\n",
    "This corresponds to the 0 index observation in our target vector:\n",
    "```\n",
    "326\n",
    "```\n",
    "\n",
    "These two separate datasets (a matrix/dataframe, and a vector), are what we will use in the `.fit(predictors, target)` function in `sklearn`'s models.  \n",
    "\n",
    "- The training data is 2D with dimensions: `n_samples x n_features`\n",
    "- The response is 1D with dimensions: `n_samples`, matching the `n_samples` of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='first-model-sklearn'></a>\n",
    "\n",
    "## Fitting our first model with `sklearn`\n",
    "\n",
    "---\n",
    "\n",
    "Now let's fit a linear regression model with the housing data. \n",
    "\n",
    "First let's visually identify some predictors that seem to have a relationship with price. \n",
    "\n",
    "**Plot each variable in `df` against the `target` variable.**\n",
    "\n",
    "> _Note: If for some reason scikit-learn crashes the jupyter notebook, have conda remove mkl (there's an issue with the newer build on some systems)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# get the axis of that figure\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot a scatter plot on CARAT with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# get the axis of that figure\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot a scatter plot on DEPTH with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# get the axis of that figure\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot a scatter plot on TABLE with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# get the axis of that figure\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot a scatter plot on X with our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# get the axis of that figure\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot a scatter plot on Y with our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# get the axis of that figure\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot a scatter plot on Z with our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we fit a linear regression model predicting `price` (the target vector) from `carat`.**\n",
    "\n",
    "> **Note:** sklearn models expect the predictor matrix to be 2D and the target to be 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions for the X matrix using `.predict(X)`, and score the model ($R^2$) using `model.score(X, y)`.**\n",
    "\n",
    "Plot the predicted values against the true values of the target, and print the model $R^2$.\n",
    "\n",
    "> **`.score(predictors, target)`**: a class method / function that returns the coefficient of determination R^2 of the prediction (for regression models).  Found in many models in scikit-learn (but not all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions  =  model.predict(X)\n",
    "score        =  model.score(X, y)\n",
    "\n",
    "# Plot the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can this plot tell us about the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-attributes'></a>\n",
    "\n",
    "## sklearn model class attributes\n",
    "\n",
    "---\n",
    "\n",
    "After you run `.fit()`, a sklearn model object often contains a variety of calculated metrics, coefficients, and other information. Which metrics and attributes are present will depend on the model – consult the documentation for specifics. \n",
    "\n",
    "Attributes in the `LinearRegression` object include:\n",
    "- **`.coef_`**: property containing the coeffients for the predictor variables\n",
    "- **`.intercept_`**: value of the intercept\n",
    "\n",
    "**Print out the beta coefficient and intercept for the model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the coefficient mean in the context of your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-metrics'></a>\n",
    "\n",
    "## Review: common metrics for evaluating regression models\n",
    "\n",
    "---\n",
    "\n",
    "The [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is a standard measure of model performance. It is the square root of the mean of the sum of squared residuals:\n",
    "\n",
    "### $$ \\operatorname{RMSE}= \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2} $$\n",
    "\n",
    "The smaller the root mean squared error, the better your model fits the data. \n",
    "\n",
    "You are already familiar with the [coefficient of determination $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination):\n",
    "\n",
    "### $$ R^2 = 1 - \\frac{SS_{reg}}{SS_{tot}} $$\n",
    "\n",
    "Where the regression sum of squares is the sum of squared residuals for our model:\n",
    "\n",
    "$SS_{reg}=\\sum_i (\\hat{y} -\\bar{y})^2$\n",
    "\n",
    "And the total sum of squares is the sum of squared residuals for the *baseline* model. This is essentially the variance of our target.\n",
    "\n",
    "$SS_{tot} = \\sum_i (y_i-\\bar{y})^2$\n",
    "\n",
    "$R^2$ is the most common metric to evaluate a regression and is the default scoring measure in sklearn. When we cover classification models, the `.score` function instead defaults to accuracy.\n",
    "\n",
    "\n",
    "**Calculate the RMSE of your model by leveraging `sklearn.metrics.mean_squared_error`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(np.sqrt(mean_squared_error(target, predictions)))\n",
    "print(r2_score(target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mlr-sklearn'></a>\n",
    "\n",
    "## Fit a MLR using sklearn\n",
    "\n",
    "---\n",
    "\n",
    "We have fit a simple linear regression predicting `price ~ carat + 1` (where the 1 represents the intercept). Use the same sklearn process and `LinearRegression` model to estimate the target with both `carat` and `table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "X = df[['carat','table']].values\n",
    "y = target\n",
    "\n",
    "model = lm.fit(X, y)\n",
    "\n",
    "predictions  =  model.predict(X)\n",
    "score        =  model.score(X, y)\n",
    "\n",
    "# Plot the model\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicted Values from CARAT + TABLE - $\\hat{y}$\")\n",
    "plt.ylabel(\"Actual Values PRICE - y\")\n",
    "\n",
    "plt.plot([0, np.max(y)], [0, np.max(y)], c = 'k')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the coefficients from this MLR model and interpret them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='negative-r2'></a>\n",
    "\n",
    "## A note on negative $R^2$ values\n",
    "\n",
    "---\n",
    "\n",
    "Over the course of this class you will encounter negative $R^2$ values. This may seem impossible, and it is in the standard scenario where we are calculating the $R^2$ score on the data we fit the model with.\n",
    "\n",
    "However, if you fit your model on one sample of data, *then score the model on new data not used to fit the model*, it is possible to end up with negative $R^2$.\n",
    "\n",
    "**What does it mean to have a negative $R^2$?**\n",
    "\n",
    "Remember that $R^2$ is 1 minus the error of your regression model divided by the error of the baseline model. A negative $R^2$ means that the regression model is performing *worse* than the baseline model. In the context of fitting our data on one sample of data and scoring on another sample, this means that we would have been better off making predictions on the test sample just using the mean of the target variable in our training set.\n",
    "\n",
    "We will return to the topic of negative $R^2$ when we talk about training and testing sets and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='statsmodels-intro'></a>\n",
    "\n",
    "## Fitting a linear regression using `statsmodels`\n",
    "\n",
    "---\n",
    "\n",
    "Now we will fit the linear regression model predicting the target from `RM` and `LSTAT`, but this time using `statsmodels`.\n",
    "\n",
    "The format looks like:\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = df[[['carat','table']].values\n",
    "# manually add the intercept column:\n",
    "X = sm.add_constant(X)\n",
    "y = target\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "model = model.fit()\n",
    "predictions = model.predict()\n",
    "```\n",
    "\n",
    "First we load the statsmodels api module, which contains the ordinary least squares `OLS` model class. The statsmodels process is slightly different:\n",
    "- We manually make a new column for the intercept in our design matrix $X$.\n",
    "- The $y$ target variable comes before the $X$ predictor\n",
    "- The data is provided during the instantiation of the model object, then fit is called without the data.\n",
    "\n",
    "**Fit the model using statsmodels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='statsmodels-summary'></a>\n",
    "\n",
    "### Statsmodels `.summary()`  function\n",
    "\n",
    "Once a model is fit with statsmodels, you can print out a variety of summary statistics, metrics, and properties of the model using the `model.summary()` function.\n",
    "\n",
    "You are already familiar with some of the information available in the summary:\n",
    "- R-squared\n",
    "- Number of observations\n",
    "- Coefficients for the variables and the intercept (const)\n",
    "- Standard errors of the coefficients, t-statistics, p-values, and confidence intervals\n",
    "\n",
    "There is also a variety of different metrics that we have not yet talked about. Don't hesitate to look up any of the statistics online if you are curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='independent-practice'></a>\n",
    "\n",
    "## Independent practice\n",
    "\n",
    "---\n",
    "\n",
    "Using either scikit-learn or statsmodels (or both, if you prefer), build a model using any set of **continuous** variables of your choice. Evaluate your model using $R^2$. Describe what the $R^2$ means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='patsy'></a>\n",
    "\n",
    "## A brief introduction to `patsy` formulas (optional)\n",
    "\n",
    "---\n",
    "\n",
    "Why slice and dice the data yourself when you just write a formula that defines your model?\n",
    "\n",
    "The `patsy` package allows you to specify the construction of your model using a formula string, and then returns the matrices required to fit the model.\n",
    "\n",
    "Let's say we wanted to predict `target` from `x`, `y` and `z`. We would write a string formula like so:\n",
    "\n",
    "```\n",
    "formula = 'target ~ x + y + z'\n",
    "```\n",
    "\n",
    "Then, after importing patsy, we can generate our target and predictor matrix by supplying the formula and the dataframe that contains the corresponding columns.\n",
    "\n",
    "```python\n",
    "import patsy\n",
    "\n",
    "y, X = patsy.dmatrices(formula, data=df, return_type='dataframe')\n",
    "```\n",
    "\n",
    "Note that all of the columns (including the target!) would need to be in `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "formula = 'target ~ x + y + z'\n",
    "\n",
    "y, X = patsy.dmatrices(formula, data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that with `return_type='dataframe'` patsy's `.dmatrices()` function returns two pandas dataframes, one for the target and one for the design matrix. \n",
    "\n",
    "You'll also notice that it creates an intercept column by default. **If you do not want it to create an intercept column, add a -1 to the formula string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formula = 'target ~ x + y + z - 1'\n",
    "\n",
    "y, X = patsy.dmatrices(formula, data=df, return_type='dataframe')\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then feed in these matrices into statsmodels or sklearn. It is generally a good practice to convert your target matrix into a 1D vector, especially when using sklearn.\n",
    "\n",
    "> **Tip:** The `.ravel()` function for numpy arrays will \"unravel\" a multidimensional matrix into a one dimensional vector of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "formula = 'target ~ x + y + z'\n",
    "\n",
    "y, X = patsy.dmatrices(formula, data=df, return_type='dataframe')\n",
    "\n",
    "y = y.values.ravel()\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
