{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Cross-Validation Lesson\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Joseph Nelson (DC), Kiefer Katovich (SF), Riley Dallas(AUS)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- **Describe** train/test split and cross-validation.\n",
    "- **Explain** how these validation techniques differ and why we want to use them.\n",
    "- **Split** data into testing and training sets using both train/test split and cross-validation and **apply** both techniques to score a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://tomrobertshaw.net/img/2015/12/overfitting.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with the first model?**\n",
    "- The underfit model falls short of capturing the complexity of the \"true model\" of the data.\n",
    "\n",
    "**What's wrong with the third model?**\n",
    "- The overfit model is too complex and is modeling random noise in the data.\n",
    "\n",
    "**The middle model is a good compromise.**\n",
    "- It approximates the complexity of the true model and does not model random noise in our sample as true relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "---\n",
    "\n",
    "We'll need the following libraries for today's lesson:\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `seaborn`\n",
    "- we'll need a `LinearRegression` from `sklearn.linear_model`\n",
    "- from `sklearn.model_selection` we'll need `train_test_split`, `KFold`, and `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "---\n",
    "\n",
    "Today's [dataset](http://www-bcf.usc.edu/~gareth/ISL/data.html) (`Advertising.csv`) is from the [ISLR website](http://www-bcf.usc.edu/~gareth/ISL/). \n",
    "\n",
    "Drop `Unnamed: 0` once you've loaded the csv into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "---\n",
    "\n",
    "Check the following in the cells below:\n",
    "1. Do we have any null values?\n",
    "2. Do we have all numerical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Plot a Heatmap of the Correlation Matrix\n",
    "---\n",
    "\n",
    "Heatmaps are an effective way to visually examine the correlational structure of your predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Use seaborn's `.pairplot()` method to create scatterplots for each of our columns\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='x-y'></a>\n",
    "\n",
    "## Create our features matrix (`X`) and target vector (`y`)\n",
    "---\n",
    "\n",
    "The following columns will be our features:\n",
    "- `TV`\n",
    "- `radio`\n",
    "- `newspaper`\n",
    "\n",
    "The `sales` column is our label: the column we're trying to predict.\n",
    "\n",
    "In the cell below, create your `X` and `y` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train-test-split\"></a>\n",
    "## Train/Test Split and Model Validation\n",
    "\n",
    "---\n",
    "\n",
    "So far we've focused on fitting the best model to our data. But is this the best model for our sample data or the best model overall? How do we know?\n",
    "\n",
    "In practice we need to validate our model's ability to generalize to new data. One popular method for performing model validation is by splitting our data into subsets: data on which we *train* our model and data on which we *test* our model.\n",
    "\n",
    "The most basic type of \"hold-out\" validation is called **train/test split**. We split our data into two pieces:\n",
    "\n",
    "> **\"A Training Set\":** The subset of the data on which we fit our model.\n",
    "\n",
    "> **\"A Testing Set\":** The subset of the data on which we evaluate the quality of our predictions.\n",
    "\n",
    "\n",
    "**Train/Test Split Benefits:**\n",
    "\n",
    "- Testing data can represent \"future\" data; for prediction-oriented models, it's critical to ensure that a model that is performing well on current data will likely perform well on future data.\n",
    "- It can help diagnose and avoid overfitting via model tuning.\n",
    "- It can improve the quality of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-tts'></a>\n",
    "\n",
    "## Scikit-Learn's `train_test_split` Function\n",
    "---\n",
    "\n",
    "Performing train/test splits using scikit-learn is easy — load the `train_test_split` function:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "**Arguments**:\n",
    "- *Arrays*: Any number of arrays/matrices to split up into training and testing sets (they should be the same length).\n",
    "- `test_size`: An integer representing the exact size of the testing subset or a float for a percentage.\n",
    "- `train_size`: Alternatively, you can specify the training size.\n",
    "- `stratify`: Supply a vector to stratify the splitting (by more important classification tasks).\n",
    "- `random_state`: a numerical seed for randomly splitting your data with reproducibility\n",
    "\n",
    "**Perform a split of our `X` and `y`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression model\n",
    "---\n",
    "\n",
    "In the cells below:\n",
    "1. create a `LinearRegression` model\n",
    "2. fit it to your **training data** (`X_train`, `y_train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "---\n",
    "\n",
    "We need two scores to determine if our model is performing well, or experiencing high bias/variance. Use the `.score()` method on **both** the training set (`X_train`, `y_train`) and the test set (`X_test`, `y_test`).\n",
    "\n",
    "```python\n",
    "model.score(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross-val-k-fold'></a>\n",
    "\n",
    "## K-Fold Cross-Validation\n",
    "\n",
    "---\n",
    "\n",
    "K-fold cross-validation takes the idea of a single train/test split and expands it to *multiple tests* across different train/test splits of your data.\n",
    "\n",
    "For example, if you determine your training set will contain 80 percent of the data and your testing set will contain the other 20 percent, you could have five different 80/20 splits in which the test set in each is a different set of observations. We have:\n",
    "- Five (K=5) training sets.\n",
    "- Five (K=5) corresponding testing sets.\n",
    "\n",
    "**K-fold cross-validation builds K models — one for each train/test pair — and evaluates those models on each respective test set.**\n",
    "\n",
    "### K-Fold Cross-Validation Visually\n",
    "\n",
    "<img src=\"https://snag.gy/o1lLcw.jpg?convert_to_webp=true\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "Cross-validation helps us understand how a model parameterization may perform in a variety of cases. The k-fold cross-validation procedure can be described in pseudocode:\n",
    "\n",
    "```\n",
    "set k\n",
    "create k groups of rows in data\n",
    "\n",
    "for group i in k row groups:\n",
    "    test data is data[group i]\n",
    "    train data is data[all groups not i]\n",
    "    \n",
    "    fit model on train data\n",
    "    \n",
    "    score model on test data\n",
    "    \n",
    "evaluate mean of k model scores\n",
    "evaluate variance of k model scores\n",
    "```\n",
    "\n",
    "Odd case No. 1:\n",
    "> **When K=2**: This is equivalent to performing ***two*** mirror image 50-50 train/test splits.\n",
    "\n",
    "Odd case No. 2:\n",
    "> **When K=number of rows**: This is known as \"leave-one-out cross-validation,\" or LOOCV. A model is built on all but one row and tested on the single excluded observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Folds Cross Validation in `sklearn`\n",
    "---\n",
    "\n",
    "Now let's try out k-fold cross-validation. Again, scikit-learn provides useful functions to do the heavy lifting. \n",
    "\n",
    "The function `cross_val_score` returns the $R^2$ for each testing set. \n",
    "\n",
    "In the cells below, score your training set (`X_train`, `y_train`) using `cross_val_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold challenge:\n",
    "---\n",
    "\n",
    "In the cell bellow, manually recreate the results from `cross_val_score` using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Cross-Validation Example](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py).\n",
    "- [Plotting Cross-Validated Predictions](http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html).\n",
    "- Review this [academic paper](http://frostiebek.free.fr/docs/Machine%20Learning/validation-1.pdf) on the underpinnings of the hold-out method, LOOCV, and k-folds.\n",
    "- Review the scikit-learn [documentation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) on cross-validation.\n",
    "- Review this [Stanford lesson](https://www.youtube.com/watch?v=_2ij6eaaSl0) on cross-validation.\n",
    "- Review this [blog post](http://www.win-vector.com/blog/2015/01/random-testtrain-split-is-not-always-enough/) on why TTS is not always sufficient.\n",
    "- Review this Stack Exchange [discussion](http://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio) on approximate TTS and validation set sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
