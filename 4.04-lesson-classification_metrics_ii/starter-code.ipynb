{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Classification Metrics II\n",
    "\n",
    "Author: Matt Brems (DC), Dave Yerrington (SF)\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Describe the inverse relationship between sensitivity and specificity.\n",
    "- Construct the ROC curve.\n",
    "- Understand how ROC AUC is calculated and interpret ROC AUC.\n",
    "- Identify methods for handling unbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data.\n",
    "df = pd.read_csv('./datasets/Whickham.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out first five rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `outcome`: Whether someone is alive or dead.\n",
    "- `smoker`: Whether somebody smoked or did not smoke.\n",
    "- `age`: Age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with value 1 if dead and 0 if alive.\n",
    "df['outcome_dummy'] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of our outcome_dummy variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm we created outcome_dummy properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with value 1 if smoker and 0 if non-smoker.\n",
    "df['smoker_dummy'] ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the interesting things about this data is the relationship between age and smoking.\n",
    "- You can read more about it [here](https://www2.stat.duke.edu/courses/Spring08/sta103/whickham.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we usually account for a relationship in our independent variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y.\n",
    "\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Conduct a train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic Regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model.\n",
    "lr ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read more about the solver [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View coefficients.\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret coefficient for age.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How would you interpret the coefficient for age?</summary>\n",
    "\n",
    "- As age increases by 1, someone is 1.104 times as likely to be dead.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>In this situation, what term would we use to describe someone who is predicted to be dead but actually is alive? (Remember that alive is coded as 0 and dead is coded as 1.)</summary>\n",
    "\n",
    "- We **falsely** predict someone to be **positive**.\n",
    "- This would be a **false positive**.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>In this situation, what is a true negative?</summary>\n",
    "\n",
    "- We **correctly** predict someone to be **negative**.\n",
    "- Someone who is predicted to be alive (`0`) and actually is alive (`0`).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If I want to get a good understanding of how our model will do on new data, should I generate a confusion matrix on our training or testing set? Why?</summary>\n",
    "    \n",
    "- Testing set!\n",
    "- If we generate one on our training set, we're going to overestimate the performance of our model... just like if we calculated MSE on our training set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "preds ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix.\n",
    "# Documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "confusion_matrix(, # True values.\n",
    "                 )  # Predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TN/FP/FN/TP values.\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# Note that .ravel() will arrange items in a one-dimensional array.\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the specificity of our test set?\n",
    "\n",
    "spec =\n",
    "\n",
    "print(f'Specificity: {round(spec,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the sensitivity of our test set?\n",
    "\n",
    "sens =\n",
    "\n",
    "print(f'Specificity: {round(sens,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dataframe called pred_df that contains:\n",
    "# 1. The list of true values of our test set.\n",
    "# 2. The list of predicted probabilities based on our model.\n",
    "\n",
    "pred_proba = [i[1] for i in lr.predict_proba(X_test)]\n",
    "\n",
    "pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs':pred_proba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create histogram of observations.\n",
    "plt.hist(pred_df['pred_probs'], bins=25)\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 0')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 1')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do you notice about this distribution?</summary>\n",
    "\n",
    "- Way more blue than orange.\n",
    "- There's lots of overlap!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 0')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 1')\n",
    "\n",
    "# Add vertical line at P(Outcome = 1) = 0.5.\n",
    "plt.vlines(x=0.5,\n",
    "           ymin = 0,\n",
    "           ymax = 65,\n",
    "           color='r',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Where are my true positives, true negatives, false positives, false negatives in this image?</summary>\n",
    "    \n",
    "- True Positive:\n",
    "    - Items I correctly predict to be positive.\n",
    "    - The orange bars (actual `1`) that are to the right of the red line (predicted `1`).\n",
    "- True Negative: \n",
    "    - Items I correctly predict to be negative.\n",
    "    - The blue bars (actual `0`) that are to the left of the red line (predicted `0`).\n",
    "- False Positive:\n",
    "    - Items I incorrectly predict to be positive.\n",
    "    - The blue bars (actual `0`) that are to the right of the red line (predicted `1`).\n",
    "- False Negative: \n",
    "    - Items I incorrectly predict to be negative.\n",
    "    - The orange bars (actual `1`) that are to the left of the red line (predicted `0`).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 0')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 1')\n",
    "\n",
    "# Add vertical line at P(Outcome = 1) = 0.1.\n",
    "plt.vlines(x=0.1,\n",
    "           ymin = 0,\n",
    "           ymax = 65,\n",
    "           color='r',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>When I moved my classification threshold to the left, what happened to sensitivity and specificity?</summary>\n",
    "\n",
    "- Our number of true negatives decreased and our number of false positives went increased.\n",
    "    - $\\text{Sensitivity} = \\frac{TN}{TN + FP} \\Rightarrow \\text{Sensitivity decreases.}$\n",
    "- Our number of true positives increased and our number of false negatives decreased.\n",
    "    - $\\text{Specificity} = \\frac{TP}{TP + FN} \\Rightarrow \\text{Specificity increases.}$\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "Our goal should be to build a model such that there is **no overlap** between the blue histogram and the orange histogram!\n",
    "- If there is overlap, we need to recognize the tradeoff between sensitivity and specificity. (As one increases, the other decreases.)\n",
    "- One measure of how much overlap exists between our distributions is the **area under the ROC curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The Receiver Operating Characteristic curve is a way to visualize the overlap between our positive class and negative class by moving our classification threshold from 0 to 1.\n",
    "- We start our classification threshold (dashed red line) at 0.\n",
    "- We calculate sensitivity and 1 - specificity.\n",
    "- We plot the value of (x = sensitivity, y = 1 - specificity).\n",
    "- We increase our classification threshold a small number (like 0.005).\n",
    "- We calculate sensitivity and 1 - specificity.\n",
    "- We plot the value of (x = sensitivity, y = 1 - specificity).\n",
    "- We repeat until our threshold is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values.\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Receiver Operating Characteristic Curve', fontsize=22)\n",
    "plt.ylabel('Sensitivity', fontsize=18)\n",
    "plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "The more area under this blue curve is, the better separated our distributions are.\n",
    "- Check out [this gif](https://twitter.com/DrHughHarvey/status/1104435699095404544).\n",
    "\n",
    "We use the **area under the ROC curve** (abbreviated **ROC AUC** or **AUC ROC**) to quantify the gap between our distributions.\n",
    "\n",
    "<details><summary>Note: Not to be confused with the ROC AOC.</summary>\n",
    "<img src=\"./images/AOC.jpg\" alt=\"AOC\" width=\"400\"/>\n",
    "Disclaimer: AOC ROC is not a meaningful metric.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roc_auc_score.\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate score.\n",
    "roc_auc_score(pred_df['true_values'], pred_df['pred_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values. (Dashed red line in image.)\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title(f'ROC Curve with AUC = {round(roc_auc_score(pred_df['true_values'], pred_df['pred_probs']),3)}', fontsize=22)\n",
    "plt.ylabel('Sensitivity', fontsize=18)\n",
    "plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting ROC AUC\n",
    "- If you have an ROC AUC of 0.5, your positive and negative populations perfectly overlap and your model is as bad as it can get.\n",
    "- If you have an ROC AUC of 1, your positive and negative populations are perfectly separated and your model is as good as it can get.\n",
    "- The closer your ROC AUC is to 1, the better. (1 is the maximum score.)\n",
    "- If you have an ROC AUC of below 0.5, your positive and negative distributions have flipped sides. By flipping your predicted values (i.e. flipping predicted 1s and 0s), your ROC AUC will now be above 0.5.\n",
    "    - Example: You have an ROC AUC of 0.2. If you change your predicted 1s to 0s and your predicted 0s to 1s, your ROC AUC will now be 0.8!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate one ROC curve per model. The ROC curve is generated by varying our threshold from 0 to 1. This doesn't actually change the threshold or our original predictions, but it helps us to visualize our tradeoff between _sensitivity_ and _specificity_ and understand how well-separated our populations are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced / Unbalanced Classes\n",
    "\n",
    "Suppose I want to predict the incidence of West Nile Virus (WNV) in Chicago.\n",
    "- 99.9% of my observations are \"no WNV.\"\n",
    "- 0.1% of my observations contain \"WNV.\"\n",
    "\n",
    "If we fit a model and tried to optimize for accuracy, I can predict \"no WNV\" for every location and have an accuracy score that is really, _really_ good!\n",
    "\n",
    "<details><summary>Why is this a bad model?</summary>\n",
    "    \n",
    "- We'll never predict that a location has West Nile Virus, which is probably going to lead to outbreaks of the disease.\n",
    "</details>\n",
    "\n",
    "In classification problems, methods generally better when we have roughly equally-sized classes. (i.e. 50% in the positive class and 50% in the negative class for binary classification problems).\n",
    "\n",
    "#### Methods for Dealing with Unbalanced Classes\n",
    "\n",
    "- **Bias correction.** Gary King wrote a [great whitepaper](https://gking.harvard.edu/files/gking/files/0s.pdf) on this topic.  This is a rigorous approach and while provide good results, as data scientists we prefer \"easier\" methods.\n",
    "\n",
    "\n",
    "- **Weighting observations.** Some models allow the weighting of classes such as linear and logistic regression, Naive Bayes, Random Forests, SVMs, etc.\n",
    "\n",
    "\n",
    "- **Stratified cross-validation.** If we use $k$-fold cross-validation entirely randomly, we may run into issues where some of our folds have no observations from the minority class.\n",
    "\n",
    "<img src=\"https://snag.gy/PqISr3.jpg\">\n",
    "\n",
    "By stratifying on our output variable with unbalanced classes during cross- validation, we protect ourselves from this situation and ensure that our estimate of our model performance has lower variance.\n",
    "\n",
    "\n",
    "- **Changing threshold for classification.**\n",
    "By adjusting our classification threshold, we might find a better fit for our particular use-case.  We performed this operation earlier by manually changing our predicted labels by choosing a new threshold from the predicted probability.\n",
    "\n",
    "\n",
    "- **Purposefully optimizing evaluation metrics.**\n",
    "We might also consider optimizating our model for a specific metric such as precision, recall by class, ROC AUC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
