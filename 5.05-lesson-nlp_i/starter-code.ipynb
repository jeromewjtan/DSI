{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![GA Logo](https://camo.githubusercontent.com/6ce15b81c1f06d716d753a61f5db22375fa684da/68747470733a2f2f67612d646173682e73332e616d617a6f6e6177732e636f6d2f70726f64756374696f6e2f6173736574732f6c6f676f2d39663838616536633963333837313639306533333238306663663535376633332e706e67) \n",
    "\n",
    "## NLP I: Tokenizing/Lemmatization and Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we begin, try running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran into issues with the above:\n",
    "\n",
    "1. Run `nltk.download()`. A new screen will pop up outside your Jupyter notebook. (It may be hidden behind other windows.)\n",
    "2. Once this box opens up, click `all`, then `download`. Once this is done, restart your Jupyter notebook and try running the first three cells again.\n",
    "3. Run:\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")```\n",
    "\n",
    "    - If this returns `cat`, then fantastic! You’re done. \n",
    "    - If not, head to http://www.nltk.org/install.html and follow instructions for your computer, then try running the first three cells again.\n",
    "    \n",
    "#### Also run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of these was machine generated?\n",
    "\n",
    "- A: \"Kilimanjaro is a mountain of 19,710 feet covered with snow and is said to be the highest mountain in Africa. The summit of the west is called “Ngaje Ngai” in Masai, the house of God. Near the top of the west there is a dry and frozen dead body of leopard. No one has ever explained what leopard wanted at that altitude.\"\n",
    "\n",
    "- B: \"Kilimanjaro is a snow-covered mountain 19,710 feet high, and is said to be the highest mountain in Africa. Its western summit is called the Masai “Ngaje Ngai,” the House of God. Close to the western summit there is the dried and frozen carcass of a leopard. No one has explained what the leopard was seeking at that altitude.\"\n",
    "\n",
    "<details><summary>Answer:</summary>\n",
    "\n",
    "- Item B was written by [Ernest Hemingway](https://en.wikipedia.org/wiki/Ernest_Hemingway) in \"The Snows of Kilimanjaro.\"\n",
    "\n",
    "- Item A was produced by a Japanese author translating \"The Snows of Kilimanjaro\" into Japanese, then this Japanese version was passed through Google Translate so that it could be \"translated back\" into English.\n",
    "</details>\n",
    "\n",
    "**Natural language processing** (NLP) describes the field of getting computers to understand language how we as humans do. Natural language processing has many, many applications including:\n",
    "- voice-to-text services for people who are hard of hearing.\n",
    "- text-to-voice services for people who have difficulty reading.\n",
    "- automated chatbots for organizations.\n",
    "- translation services.\n",
    "\n",
    "Generally when we get text data, strings aren't broken out into individual words or even sentences. We might have a full tweet, full chapter of a book, or full .pdf file all in one long string.\n",
    "\n",
    "Today, we're diving into the practical side of NLP - taking text data and breaking it out into words that we can then leverage into $n$-grams or $tf$-$idf$ vectorizers.\n",
    "\n",
    "## Agenda\n",
    "1. Pre-Processing\n",
    "2. Sentiment Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Define** and **implement** tokenizing, lemmatizing, and stemming.\n",
    "2. **Describe** what RegEx does.\n",
    "3. **Apply** sentiment analysis.\n",
    "4. **Preprocess** text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spam text.\n",
    "spam = 'Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.'\n",
    "\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing \n",
    "\n",
    "When dealing with text data, there are some common pre-processing steps we might use. However, we won't necessarily use all of them every time we deal with text data.\n",
    "\n",
    "- Tokenizing\n",
    "- Regular Expression\n",
    "- Lemmatizing/Stemming\n",
    "- Cleaning (i.e. removing HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "When we \"tokenize\" data, we take it and split it up into distinct chunks based on some pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') ## We'll talk about this in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Run\" Tokenizer\n",
    "spam_tokens ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In comparing the original text to our tokenized version of the text, we converted one long string into a list of strings. What other changes occurred?</summary>\n",
    "\n",
    "- All strings were converted to lower case.\n",
    "- All punctuation was removed.\n",
    "- This was done using **regular expressions**.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "Regular Expressions, or RegEx, are an extraordinarily helpful way for us to detect patterns in text. \n",
    "- This is a tool of which you should be aware, but you'll learn more about it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in spam_tokens:\n",
    "    print(re.findall('\\d+', i), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegEx in Python 3 understands `\\d+` to identify numeric digits. Therefore, the above code searched through `spam_tokens` to see if any numeric digits were in there. \n",
    "\n",
    "A `RegexpTokenizer` splits a string into substrings using regular expressions.\n",
    "\n",
    "The following example is pulled from [this site](http://www.nltk.org/_modules/nltk/tokenize/regexp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and print string.\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_1 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_1` splits tokens up by spaces or by periods that are not attached to a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_2 = RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "# Run tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_2` will identify the spaces. By setting `gaps = True`, we're grabbing everything else: thus, we're splitting our tokens up by spaces.\n",
    "- If you changed to `gaps = False`, you'll return only the whitespaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_3 = RegexpTokenizer('[A-Z]\\w+')\n",
    "\n",
    "# Run tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_3` returns only words that begin with a capital letter.\n",
    "\n",
    "As you can imagine, using RegEx can be incredibly helpful if you want to find text matching a specific pattern.\n",
    "- People used to use two spaces after a period to split sentences up; you could use RegEx to detect that pattern and tokenize on entire sentences.\n",
    "- Chapters in a book could be titled \"Chapter\" followed by a number; you could use RegEx to detect that pattern and tokenize a book by its chapters.\n",
    "- When Python libraries are upgraded, syntax changes! Perhaps you want to detect a certain pattern of syntax so you can update your code efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing & Stemming\n",
    "\n",
    "- \"He is *running* really fast!\"\n",
    "- \"He *ran* the race.\"\n",
    "- \"He *runs* a five-minute mile.\"\n",
    "\n",
    "If we wanted a computer to interpret these sentences, I might count up how many instances of each word I observe. The computer will treat words like \"running,\" \"ran,\" and \"runs\" differently although they mean about the same thing (in this context).\n",
    "\n",
    "**Lemmatizing** and **stemming** are two forms of shortening words so we can combine similar forms of the same word.\n",
    "\n",
    "When we \"lemmatize\" data, we take words and attempt to return their *lemma*, or the base/dictionary form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lemmatizer. (Same as above.)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate lemmatizer. (Same as above.)\n",
    "lemmatizer ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "tokens_lem ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare tokens to lemmatized version.\n",
    "list(zip(spam_tokens, tokens_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only those lemmatized tokens that are different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing is usually the more correct and precise way of handling things from a grammatical/morphological point of view, but also might not have much of an effect. \n",
    "\n",
    "We can also do this on individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the word \"computers.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we \"stem\" data, we take words and attempt to return a base form of the word. It tends to be cruder than using lemmatization. There's a [method developed by Porter in 1980](https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf) that explains the algorithm used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate stemmer.\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate object of class PorterStemmer.\n",
    "p_stemmer ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem tokens.\n",
    "stem_spam ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokens to stemmed version.\n",
    "list(zip(spam_tokens, stem_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only those stemmed tokens that are different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this on individual words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the word \"computer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the word \"computation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the word \"computationally.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start with a very simple example.\n",
    "\n",
    "[Sentiment analysis](https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html) is an area of natural language processing in which we seek to classify text as having positive or negative emotion.\n",
    "\n",
    "Let's build a simple function that can classify text as either having positive or negative sentiment.\n",
    "\n",
    "What words tell us whether certain text is positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's come up with a list of positive and negative words we might observe.\n",
    "\n",
    "positive_words =\n",
    "negative_words ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T10:03:50.010475Z",
     "start_time": "2017-10-25T10:03:50.006364Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_sentiment(text):\n",
    "    # Instantiate tokenizer.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # Tokenize text.\n",
    "    tokens = \n",
    "    \n",
    "    # Instantiate stemmer.\n",
    "    p_stemmer = \n",
    "    \n",
    "    # Stem words.\n",
    "    stemmed_words =\n",
    "    \n",
    "    # Stem our positive/negative words.\n",
    "    positive_stems =\n",
    "    negative_stems =\n",
    "\n",
    "    # Count \"positive\" words.\n",
    "    positive_count =\n",
    "    \n",
    "    # Count \"negative\" words\n",
    "    negative_count =\n",
    "    \n",
    "    # Calculate Sentiment Percentage \n",
    "    # (Positive Count - Negative Count) / (Total Count)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our sentiment analyzer on our spam email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three not-so-random Chipotle reviews.\n",
    "\n",
    "yelp_1 = \"No Chipotle should ever have a 2 out of 5 star rating on Yelp. Especially not this one. As a regular (usually two or three visits a week), I have never been dissatisfied with a single meal here. It's Chipotle, so you know you'll pay $8 (after tax) for a chicken bowl and be full and satisfied afterwards. \\n The employees are friendly and give generous portions. Seating is limited, but there is a place you can stand and eat near the window, which is where I always eat. I'm sitting down eight hours a day at the office anyway - standing and eating here is probably extending my lifespan. \\nThe line gets line long during lunch, but it moves fast. Dinner time is amazing - rarely a line and the portions are extra generous during this time.\\n This fairly new Chipotle is at a great location, near McPherson Square. It's right next to my office and gym so it's perfect for me. \\nBottom line: if you're craving Chipotle and are worried about the other reviews and low ratings for this location, don't be. It's my favorite Chipotle location in the DC area, and that's not an exaggeration.\"\n",
    "\n",
    "yelp_2 = \"DISGUSTING LONG HAIR THREADED THROUGH CHICKEN IN BURRITO BOWL.\\n There was a long blonde hair threaded through my chicken as I was eating a burrito bowl.  I did not notice until it was too late and the HAIR ENTERED MY MOUTH as I was eating and I grossly pulled the hair out.\\n I calmly walked up to the register to inform them that there was hair in my food. The register person was busy, I understand that, but I was promptly ignored like my issue was not a big deal.  He proceeded to get his manager, 'Leslie' I believe.  She was not apologetic at all and offered no condolences. She did however offer a refund, but I didn't care about the money, I just wanted to eat food without eating someone's hair as a side dish.\\n The second time I went back up, a different person, the general manager Peris, was more apologetic and handled the situation better. He ended up getting Leslie to file a report, but who knows if they submitted it or not.\\n Suffice to say, if you dont want food in your hair, dont eat here.\"\n",
    "\n",
    "yelp_3 = \"First time going to this Chipotle.  The line was very quick and the food was fresh.  But as I started eating a notice that the food was very salty.  I started separating my bowl after two bites.  I ordered a bowl with white rice, black beans, chicken, sour cream, cheese and lettuce.  I tasted everything separately.  Once I tasted the Chicken by it self it was unbearable.  It taste like someone pouring the entire bottle of salt on tge chicken.  I tried to take most the chicken out the bowl but still I could not bear the taste of the salt.  So I ended up throwing the damn bowl away.  $8.00 down the drain.  SMH.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment of yelp_1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment of yelp_2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment of yelp_3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> What are some shortcomings of this method? </summary>\n",
    "\n",
    "- Primarily, we're limited to the positive/negative words we came up with.\n",
    "- If someone wrote \"not good\" or \"not bad,\" our sentiment function would probably treat \"not good\" as positive or neutral instead of negative.\n",
    "- The ordering of the words doesn't matter here, which is not how language generally works.\n",
    "- We haven't been able to correct for misspellings.\n",
    "</details>\n",
    "\n",
    "# Sorting Positive from Negative Reviews\n",
    "\n",
    "The easiest way to do sentiment classification of analysis is by training a model on data we've already labeled.\n",
    "- This is a huge consideration for your capstone, even if it isn't related to NLP!\n",
    "\n",
    "Today, we will begin by reviewing the basic NLP techniques we learned yesterday to create a sentiment analyzer from Rotten Tomatoes Movie review. This code-along is adapted from Kaggle's tutorial, available [here](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words).\n",
    "\n",
    "\n",
    "## Step One: Import The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.099103Z",
     "start_time": "2017-10-24T15:40:05.659674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import pandas.\n",
    "import pandas as pd       \n",
    "\n",
    "# Read in training data.\n",
    "train = pd.read_csv(\"../labeledTrainData.tsv\",\n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.680295Z",
     "start_time": "2017-10-24T15:40:07.659485Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the first five rows.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data.\n",
    "test = pd.read_csv(\"../testData.tsv\",\n",
    "                   header=0, \n",
    "                   delimiter=\"\\t\",\n",
    "                   quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows.\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our Kaggle data was organized `train.csv` and `test.csv`. However, the `test.csv` didn't contain any labels!\n",
    "\n",
    "Let's do a train/test split by splitting up `train.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[['id','review']],\n",
    "                                                    train['sentiment'],\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few steps we'll take to clean up the text data before it's ready for processing.\n",
    "\n",
    "- Remove the HTML code artifacts from the text.\n",
    "- Remove punctuation.\n",
    "- Remove stopwords. (We'll cover these shortly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step One: Remove HTML code artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T10:26:41.848053Z",
     "start_time": "2017-10-25T10:26:41.753460Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "# Initialize the BeautifulSoup object on a single movie review     \n",
    "example1 = BeautifulSoup(X_train['review'][0])\n",
    "\n",
    "# Print the raw review and then the output of get_text(), for \n",
    "# comparison\n",
    "print(X_train['review'][0])\n",
    "print()\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Two: Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation can be removed using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:27.150446Z",
     "start_time": "2017-10-24T15:40:27.144047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text())   # The text to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T17:03:08.878606Z",
     "start_time": "2017-10-24T17:03:08.862224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show first fifty letters of letters_only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:31.298135Z",
     "start_time": "2017-10-24T15:40:31.294230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert letters_only to lower case.\n",
    "lower_case =\n",
    "\n",
    "# Split lower_case up at each space.\n",
    "words = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first ten words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three: Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our Yelp reviews above, you noticed that our sentiment scores were right around zero. While there were some positive and negative words, the vast majority of the words had neither a positive sentiment nor negative sentiment!\n",
    "- Examples include \"the,\" \"of,\" \"and,\" \"a,\" \"to,\" and \"in.\"\n",
    "    \n",
    "Stopwords are very common words that are often removed because they amount to unnecessary information and removing them can dramatically speed up processing.\n",
    "\n",
    "If you didn't complete the NLTK download, you may run into some issues here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:35.652249Z",
     "start_time": "2017-10-24T15:40:34.428977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download text data sets, including stop words. Uncomment this if you did not download!\n",
    "\n",
    "# nltk.download()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T19:10:56.773781Z",
     "start_time": "2017-10-24T19:10:56.736190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import stopwords.\n",
    "from nltk.corpus import stopwords # Import the stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print English stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:44.470722Z",
     "start_time": "2017-10-24T15:40:44.394320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove stop words from \"words.\"\n",
    "words ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check \"words\" to make sure we did this properly.\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Four: Combine our cleaning into one function\n",
    "\n",
    "**Check**: Why should we do everything with one function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:41:00.960134Z",
     "start_time": "2017-10-24T15:41:00.951205Z"
    }
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    # Notice that we did this in one line!\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stop words to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Remove stop words.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Five (Finally!) Applying our Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:41:04.730461Z",
     "start_time": "2017-10-24T15:41:04.715975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe size.\n",
    "total_reviews = train.shape[0]\n",
    "print(f'There are {total_reviews} reviews.')\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews.\n",
    "clean_train_reviews = []\n",
    "clean_test_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:41:46.357830Z",
     "start_time": "2017-10-24T15:41:20.317753Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Cleaning and parsing the training set movie reviews...\")\n",
    "\n",
    "j = 0\n",
    "for train_review in X_train['review']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_train_reviews.append(review_to_words(train_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "\n",
    "print(\"Cleaning and parsing the testing set movie reviews...\")\n",
    "\n",
    "for test_review in X_test['review']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_test_reviews.append(review_to_words(test_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "        \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data is finally ready....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:42:31.247194Z",
     "start_time": "2017-10-24T15:42:27.791843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll describe this in greater detail in a later lesson, but CountVectorizer will transform the lists of the cleaned reviews above into features that we can pass into a model.\n",
    "- It will create columns (also knon as vectors), where each column counts how many times each word is observed in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array.\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:42:39.620120Z",
     "start_time": "2017-10-24T15:42:39.615238Z"
    }
   },
   "outputs": [],
   "source": [
    "print(test_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:42:42.233994Z",
     "start_time": "2017-10-24T15:42:42.222757Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have an array that we can use for classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T09:59:36.116770Z",
     "start_time": "2017-10-25T09:59:36.104354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import logistic regression.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression model.\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T10:00:13.920965Z",
     "start_time": "2017-10-25T10:00:01.171005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to training data.\n",
    "\n",
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on training data.\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is this score?</summary>\n",
    "\n",
    "- Accuracy.\n",
    "- Remember that the default metric for classification models in `scikit-learn` is accuracy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on testing data.\n",
    "\n",
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What would we conclude about our model based on these scores?</summary>\n",
    "\n",
    "- Since our model has a higher accuracy on the training set than on the testing set, our model is overfit.\n",
    "- We could combat this overfitting by removing features (i.e. setting `max_features` to be lower than 5000) and/or by regularizing our model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note:\n",
    "1. NLP broadly describes: \n",
    "    - how we can get unstructured text data into a more structured form that can be interpreted by computers and \n",
    "    - algorithms for interpreting text data.\n",
    "2. That does not mean these tools we used today work to the exclusion of other methods. You can and should include other variables in your model!\n",
    "    - For example, maybe the length of a review tells us something about how much people liked/disliked the movie, or maybe additional information about the reviewer (i.e. geography, age, how many reviews they had submitted) has predictive value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 226,
   "position": {
    "height": "40px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
