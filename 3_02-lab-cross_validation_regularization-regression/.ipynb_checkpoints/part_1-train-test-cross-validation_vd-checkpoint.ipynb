{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Train-Test Split and Cross-Validation Lab\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "## Review of Train/Test Validation Methods\n",
    "\n",
    "We've discussed overfitting, underfitting, and how to validate the \"generalizeability\" of your models by testing them on unseen data. \n",
    "\n",
    "In this lab you'll practice two related validation methods: \n",
    "1. **train/test split**\n",
    "2. **k-fold cross-validation**\n",
    "\n",
    "Train/test split and k-fold cross-validation both serve two useful purposes:\n",
    "- We prevent overfitting by not using all the data, and\n",
    "- We retain some remaining data to evaluate our model.\n",
    "\n",
    "In the case of cross-validation, the model fitting and evaluation is performed multiple times on different train/test splits of the data.\n",
    "\n",
    "Ultimately we can use the training/test validation framework to compare multiple models on the same dataset. This could be comparisons of two linear models, or of completely different models on the same data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "For your independent practice, fit **three different models** on the Boston Housing Data. For example, you could pick three different subsets of variables, one or more polynomial models, or any other model that you like. \n",
    "\n",
    "**Start with train/test split validation:**\n",
    "* Fix a testing/training split of the data.\n",
    "* Train each of your models on the training data.\n",
    "* Evaluate each of the models on the testing data.\n",
    "* Rank the models by how well they score on the testing data set.\n",
    "\n",
    "**Then try k-fold cross-validation:**\n",
    "* Perform a k-fold cross-validation and use the cross-validation scores to compare your models. Did this change your rankings?\n",
    "* Try a few different k-splits of the data for the same models.\n",
    "\n",
    "If you're interested, try a variety of response variables. We start with **MEDV** (the `.target` attribute from the data set load method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\old_vd_laptop_backup__190804\\code\\fastai_v1\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "x_train = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train, x_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7103879080674731"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = results.predict(sm.add_constant(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836295385076305"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7103879080674731\n",
      "0.7836295385076291\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "print(lm.score(X_train, y_train))\n",
    "print(lm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Clean up any data problems.\n",
    "\n",
    "Fix any problems with the data, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Select 3–4 variables with your data set on which to perform a 50-50 train/test split.\n",
    "\n",
    "- Use scikit-learn.\n",
    "- Score and plot your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Try 70-30 and 90-10.\n",
    "\n",
    "- Score and plot.  \n",
    "- How do your metrics change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Try k-folds cross-validation with k between 5–10 for your regression. \n",
    "\n",
    "- What seems optimal? \n",
    "- How do your scores change?  \n",
    "- What is the variance of scores like?\n",
    "- Try out different folds to get a sense of how this impacts your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [Bonus] Optimize the $R^2$ score.\n",
    "\n",
    "Can you optimize your R^2 by selecting the best features and validating the model using either train/test split or k-folds?\n",
    "\n",
    "Your code will need to iterate through the different combinations of predictors, cross-validate the current model parameterization, and determine which set of features performed best.\n",
    "\n",
    "The number of k-folds is up to you.\n",
    "\n",
    "> *Hint:* The `itertools` package is useful for combinations and permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Explain what could be wrong with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) [Bonus] Explore another target variable and practice Patsy formulas.\n",
    "\n",
    "Can you find another response variable, given a combination of predictors, that can be predicted accurately through the exploration of different predictors in this data set?\n",
    "\n",
    "**Try using Patsy to construct your target and predictor matrices from formula strings.**\n",
    "\n",
    "> *Tip: Check out pairplots, coefficients, and Pearson scores.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "# A:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
