{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "_Author: Kiefer Katovich (SF) with modifications by Justin Pounders._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Describe what PCA does and what it is used for in data science.\n",
    "- Compute principal components and transform data onto those components with `sklearn`.\n",
    "- Interpret PCA results graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Motivation](#motivation)\n",
    "- [What is PCA?](#whatispca)\n",
    "    - [Eigenvalues and Eigenvectors](#eigenpairs)\n",
    "    - [Principal Components](#pcs)\n",
    "- [Manual PCA Codealong](#manual-codealong)\n",
    "    - [1. Basic EDA](#basic-eda)\n",
    "    - [2. Subset and Normalize](#subset)\n",
    "    - [3. Find the Correlation Matrix](#corr)\n",
    "    - [4. Eignenvalues and Eigenvectors](#eigen)\n",
    "    - [5. Explained Variance](#var)\n",
    "    - [6. Projection Matrix W](#projection)\n",
    "    - [7. Transformed Matrix Z](#transformed)\n",
    "    - [8-12. Plot and Interpret Principal Components](#plot-components)\n",
    "- [More Reading](#more-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA = Principle Component Analysis\n",
    "\n",
    "**Goals**: \n",
    "\n",
    "- *Transform* original variable/features into new, \"high-performance\" features\n",
    "- *Reduce* the dimensionality of the data\n",
    "- *Eliminate* multicollinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"motivation\"></a>\n",
    "## Motivation\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction reduces the number of random variables that you are considering for analysis until you are left with the most important variables.\n",
    "\n",
    "> Dimensionality reduction is not an end goal in itself, but a tool to form a dataset with more parsimonious features for further visualization and/or modelling.\n",
    "\n",
    "### Reducing Colinearity in Input\n",
    "\n",
    "To get a quick summary of our data, we can calculate a covariance matrix, an unstandardized correlation matrix.\n",
    "\n",
    "The diagonal elements in a covariance matrix show us the variance of each of our features.\n",
    "\n",
    "The off-diagnal elements show the covariance, the amount of colinearity and redundancy between our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "\n",
    "Say that I want to predict **age** from *stress*, *income* and *health*.\n",
    "\n",
    "1. Three-dimensional data\n",
    "2. Multicollinearity probably exists\n",
    "\n",
    "*PCA* will give me one or two **super-predictor** variables called *components* (hopefully).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:**\n",
    "\n",
    "**What would an 'ideal' covariance matrix look like?**\n",
    "\n",
    "**What can we do to try to remove any redundancies and preserve the signal?**\n",
    "\n",
    "Enter PCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"whatispca\"></a>\n",
    "## What is PCA?\n",
    "\n",
    "---\n",
    "\n",
    "PCA is the quintessential \"dimensionality reduction\" algorithm. \n",
    "\n",
    "_Dimensionality reduction_ is the process of combining or collapsing the existing features (columns in X) into fewer features. \n",
    "\n",
    "These hopefully:\n",
    "\n",
    "- Retain the signal in the original data, and\n",
    "- Reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Essentially...**\n",
    "\n",
    "- PCA finds *linear combinations* of current predictor variables that...\n",
    "- create new \"principal components\". The principal components explain...\n",
    "- the maximum possible amount of variance in your predictors.\n",
    "\n",
    "$$ PC1 = w_{1,1}(\\text{stress}) + w_{2,1}(\\text{income}) + w_{3,1}(\\text{health})$$\n",
    "\n",
    "$$ PC2 = w_{1,2}(\\text{stress}) + w_{2,2}(\\text{income}) + w_{3,2}(\\text{health})$$\n",
    "\n",
    "$$ PC3 = w_{1,3}(\\text{stress}) + w_{2,3}(\\text{income}) + w_{3,3}(\\text{health})$$\n",
    "\n",
    "This is cool because...\n",
    "\n",
    "- $PC1$ is better than $PC2$ is better than $PC3$  \n",
    "- All of these are *uncorrelated*\n",
    "\n",
    "---\n",
    "\n",
    "**Visually...**\n",
    "\n",
    "> Think of PCA as a coordinate transformation.  The old axes are the original variables (columns). The new axes are the principal components from PCA.\n",
    "\n",
    "**The new axes (principal components) become the  most concise, informative descriptors of our data as a whole.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![orthogonal eigenvectors](./images/eigenvectors_orthogonal.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![transformed XY](./images/transformed_xy.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"pcs\"></a>\n",
    "### Principal Components\n",
    "\n",
    "---\n",
    "\n",
    "- We are looking for new *directions* in feature space\n",
    "- Each consecutive direction tries to maximize *remaining variance*\n",
    "- Each direction is *orthogonal* to all the others\n",
    "\n",
    "**These new *directions* are the \"principal components\", i.e. the new coordinate system for your data.**\n",
    "\n",
    "> Applying PCA to your data *transforms* your original data columns (variables) onto the new principal component axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Did you catch that?  The *variables* define the *cordinate system*.)\n",
    "\n",
    "(CP1) **Let's review...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The PCA transformation creates new variables that...**\n",
    "1. Optimize \"explained variance\", and\n",
    "2. Are uncorrelated.\n",
    "\n",
    "Creating these variables is a well-defined mathematical process. In essence, **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**.\n",
    "\n",
    "#### Example Continued\n",
    "\n",
    "The inputs `stress`, `income` and `health`...\n",
    "\n",
    "...can be *replaced* with 3 *new* variables...\n",
    "\n",
    "- `PC1` $\\rightarrow$ most variance\n",
    "- `PC2`\n",
    "- `PC3` $\\rightarrow$ least variance (noise?)\n",
    "\n",
    "---\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$ PC1 = w_{1,1}(\\text{stress}) + w_{2,1}(\\text{income}) + w_{3,1}(\\text{health})$$\n",
    "\n",
    "$$ PC2 = w_{1,2}(\\text{stress}) + w_{2,2}(\\text{income}) + w_{3,2}(\\text{health})$$\n",
    "\n",
    "$$ PC3 = w_{1,3}(\\text{stress}) + w_{2,3}(\\text{income}) + w_{3,3}(\\text{health})$$\n",
    "\n",
    "The weights are called *loadings*... they are coefficients indicating how heavily each of the input data are weighted\n",
    "\n",
    "e.g.\n",
    "\n",
    "$$ PC1 = 0.01(\\text{stress}) - 0.54(\\text{income}) + 0.71(\\text{health})$$\n",
    "\n",
    "> We will see how to get these values from `sklearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing variance\n",
    "\n",
    "The total variance of your data gets redistributed among the principal components:\n",
    "\n",
    "$$\\text{var}(PC1) > \\text{var}(PC2) > \\text{var}(PC3)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Interpreting PCA: Signal v. Noise\n",
    "\n",
    "> PCA attempts to *maximize signal* (high variance) while *isolating noise* (low variance)\n",
    "\n",
    "> - Most variance captured in first several principal components\n",
    "> - Noise isolated to last several principal compoments\n",
    "> - This done simultaneously across *all input variables*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating variance\n",
    "\n",
    "There is no covariance between principal components\n",
    "\n",
    "$$\\text{covar}(PC1, PC2) = 0$$\n",
    "\n",
    "$$\\text{covar}(PC1, PC3) = 0$$\n",
    "\n",
    "$$\\text{covar}(PC2, PC3) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two assumptions that PCA makes:**\n",
    "1. **Linearity:** The data does not hold nonlinear relationships.\n",
    "2. **Large variances define importance:** The dimensions are constructed to maximize remaining variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PRINCIPAL COMPONENT TRANSFORMATION OF DATA: PC1 VS PC2**\n",
    "\n",
    "[setosa.io has an extremely nice interactive visualization for PCA](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![setosa pc1](./images/setosa_pc1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(CP2) **Let's review...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down the rabit hole... (OPTIONAL)\n",
    "\n",
    "\n",
    "![](./images/math-formula-chalkboard.jpg)\n",
    "\n",
    "PCA relies on the...\n",
    "\n",
    "**Eigenvalue decomposition of the covariance matrix**\n",
    "This diagonalizes the covariance matrix, thereby eliminating covariance.\n",
    "\n",
    "**The principal component transformation**\n",
    "This transforms each input variable onto a new orthogonal basis in which the new variables are maximally variant.\n",
    "\n",
    "$$ \\mathbf{Z = XW} $$\n",
    "\n",
    "> *Introduction to Statistical Lerning* section 10.2 has a great overview that is of medium technical complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why would we want to do PCA?\n",
    "\n",
    "---\n",
    "\n",
    "- We can reduce the number of dimensions (remove less important components), while losing mostly noise rather than signal.\n",
    "- Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n",
    "- The directions of largest variance should have the highest signal-to-noise ratio.\n",
    "- Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent.\n",
    "\n",
    "---\n",
    "\n",
    "[Good paper on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "[Nice site on performing PCA](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"manual-codealong\"></a>\n",
    "## PCA Codealong\n",
    "\n",
    "---\n",
    "\n",
    "**DATA**\n",
    "\n",
    "We are going to be using a simple 75-row, 4-column dataset with demographic information. It contains:\n",
    "\n",
    "    age (limited to 20-65)\n",
    "    income\n",
    "    health (a rating on a scale of 1-100, where 100 is the best health)\n",
    "    stress (a rating on a scale of 1-100, where 100 is the most stressed)\n",
    "    \n",
    "All of the variables are continuous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "demo = pd.read_csv('./datasets/simple_demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"basic-eda\"></a>\n",
    "### 1. Basic EDA\n",
    "\n",
    "Make a seaborn pairplot for the dataset:\n",
    "\n",
    "1. age vs. income\n",
    "2. age vs. health\n",
    "3. age vs. stress\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(demo)\n",
    "g = g.map_lower(sns.regplot)    # Regression plots in lower triangle.\n",
    "g = g.map_upper(sns.kdeplot, cmap=\"Blues\", shade=True, shade_lowest=False)  # KDE plots in upper triangle.\n",
    "g = g.map_diag(plt.hist)        # Histograms along diagonal.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"subset\"></a>\n",
    "### 2. Subset and normalize\n",
    "\n",
    "Subset the data to only include:\n",
    "\n",
    "    income\n",
    "    health\n",
    "    stress\n",
    "\n",
    "We will be comparing the principal components to age specifically, so we are leaving age out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"corr\"></a>\n",
    "### 3. Calculate correlation matrix\n",
    "\n",
    "We will be using the correlation matrix to calculate the eigenvectors and eigenvalues.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"eigen\"></a>\n",
    "### 4. Calculate the principal components and associated explained variances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = demo_noage.values\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the \"components\": recall that\n",
    "\n",
    "$$ PC1 = w_{1,1}(\\text{health}) + w_{2,1}(\\text{income}) + w_{3,1}(\\text{stress})$$\n",
    "\n",
    "$$ PC2 = w_{1,2}(\\text{health}) + w_{2,2}(\\text{income}) + w_{3,2}(\\text{stress})$$\n",
    "\n",
    "$$ PC3 = w_{1,3}(\\text{health}) + w_{2,3}(\\text{income}) + w_{3,3}(\\text{stress})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"var\"></a>\n",
    "### 5. Calculate and plot the explained variance\n",
    "\n",
    "A useful measure is the **explained variance**, which is calculated from the eigenvalues. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "### $$ ExpVar_i = \\bigg(\\frac{\\text{variance of } PC_i}{\\text{total variance}}\\bigg)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = None\n",
    "print('Explained variance:            ', var_exp)\n",
    "\n",
    "cum_var_exp = None\n",
    "print('Cumulative explained variance: ', cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "# HERE IS THE BASIC PLOT\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "component_number = [1,2,3]\n",
    "plt.plot(component_number, cum_var_exp, lw=7)\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# NOW MAKE IT LOOK PRETTY\n",
    "\n",
    "# Add horizontal lines at y=0 and y=100\n",
    "plt.axhline(y=0, linewidth=5, color='grey', ls='dashed')\n",
    "plt.axhline(y=1, linewidth=3, color='grey', ls='dashed')\n",
    "\n",
    "# Set the x and y axis limits\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([1,3])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "# Label the axes\n",
    "ax.set_ylabel('cumulative variance explained', fontsize=16)\n",
    "ax.set_xlabel('component', fontsize=16)\n",
    "\n",
    "# Make the tick labels bigger\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "# Add title\n",
    "ax.set_title('component vs cumulative variance explained\\n', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"transformed\"></a>\n",
    "### 6. Construct the Transformed Data Set $Z$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the transformation here!\n",
    "Z = None \n",
    "print(type(Z))\n",
    "\n",
    "# Make a list of PC names. Should equal ['PC1', 'PC2', ...]\n",
    "features_pca = None \n",
    "\n",
    "# Turn the PCs into a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"plot-components\"></a>\n",
    "### 7. Plot Principal Component 1 vs 2\n",
    "\n",
    "PC1 is the first column in $Z$, and PC2 is the second.\n",
    "\n",
    "Notice how they are un-correlated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,7))\n",
    "\n",
    "ax = fig.gca()\n",
    "ax = sns.regplot(Z.iloc[:,0], Z.iloc[:,1],\n",
    "                 fit_reg=False, scatter_kws={'s':70}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('principal component 1', fontsize=16)\n",
    "ax.set_ylabel('principal component 2', fontsize=16)\n",
    "\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "ax.set_title('PC1 vs PC2\\n', fontsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 8. Plot age vs principal component 1 with regplot\n",
    "\n",
    "Look how tight the relationship is. PC1 took the shared variance out of income, health, and stress, which are intuitively directly related to increasing age. \n",
    "\n",
    "This principal component, or more specifically the column weighting matrix $W$, is essentially **capturing the latent age variance embedded in these variables.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,7))\n",
    "ax = fig.gca()\n",
    "ax = sns.regplot(Z.iloc[:,0], demo.age.values,\n",
    "                 fit_reg=True, scatter_kws={'s':70}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('principal component 1', fontsize=16)\n",
    "ax.set_ylabel('age', fontsize=16)\n",
    "\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "ax.set_title('PC1 vs age\\n', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 9. Concatenate PC1 and PC2 to the dataset, then melt it with PC1 and PC2 and index variables\n",
    "\n",
    "```python\n",
    "melted_df = pd.melt(df, id_vars=['PC1','PC2'])\n",
    "```\n",
    "\n",
    "**This is all for the sake of making a nice plot!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "demo_pcs = demo_noage.copy()\n",
    "demo_pcs['PC1'] = Z.iloc[:,0]\n",
    "demo_pcs['PC2'] = Z.iloc[:,1]\n",
    "\n",
    "demo_pcs = pd.melt(demo_pcs, id_vars=['PC1','PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "demo_pcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 10. Use lmplot to check out PC1 vs all 3 variables\n",
    "\n",
    "Make the `col` keyword argument \"variable\" and the `hue` keyword argument \"variable\" as well, assuming that's what you called them in the melt command (those are the defaults)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pc1 = sns.lmplot(x=\"PC1\", y=\"value\", col=\"variable\", hue=\"variable\", \n",
    "                 data=demo_pcs, sol_wrap=2, size=7, scatter_kws={'s':50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 11. Use lmplot to do the same for PC2\n",
    "\n",
    "Notice how PC2 captures the variance of income, which was not captured well by PC1. This makes sense, as the variance each principal component captures has to be orthogonal to the other components.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pc1 = sns.lmplot(x=\"PC2\", y=\"value\", col=\"variable\", hue=\"variable\", \n",
    "                 data=demo_pcs, col_wrap=2, size=7, scatter_kws={'s':50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build out a base-line linear regression model predicting `age` from `health`, `income` and `stress`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "features = ['health', 'income', 'stress']\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above model, but reduce the dimensionality to 2 (instead of 3) using PCA.  \n",
    "- How does your $R^2$ compare?\n",
    "- How were the principal components chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = ['health', 'income', 'stress']\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. PCA Variants\n",
    "\n",
    "What we have covered is _linear_ PCA on _dense_ matrices.  There are some other useful variants, however:\n",
    "\n",
    "- [`IncrementalPCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html) is a variant of PCA that does not require to store all of your data in memory.  This is good for when you have tons of data!\n",
    "- [`KernelPCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html) marries the kernel trick you saw with SVM's to PCA.  This means that you can effectively apply dimensionality reduction to nonlinear data!\n",
    "- When working with text data, [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) serves a similar role as PCA (i.e., dimensionality reduction).\n",
    "  - `TruncatedSVD` _will not_ standardize your data (this is fundamentally what makes it different than PCA).\n",
    "  - `TruncatedSVD` _will_ work directly with sparse matrices, like those you get back from NLP vectorizers.\n",
    "  - Applying `TruncatedSVD` to text counts or tf-idf matrices is called [Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis).  A variant of LSA called [Probabilistic Latent Semantic Analysis(PLSA)](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) is the non-Bayesian version of Latent Dirichlet Allocation (LDA), which is Topic Clustering in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"more-reading\"></a>\n",
    "### More useful links, reading, and references for images\n",
    "\n",
    "---\n",
    "\n",
    "[PCA 4 dummies](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n",
    "\n",
    "[Stackoverflow making sense of PCA](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n",
    "\n",
    "[PCA and spectral theorem](http://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit)\n",
    "\n",
    "[PCA in 3 steps: eigendecomposition and SVD](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)\n",
    "\n",
    "[Tutorial on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "[PCA math and examples](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easter Egg.  Like math?\n",
    "\n",
    "PCA works by taking the singular value decomposition (SVD) of the data matrix, $X$.  This is equivalent to the eigen-decomposition of the matrix $X^T X$.  \n",
    "\n",
    "Well... if the data are centered/standardized, then the matrix $X^T X$ is nothing more than the *covariance matrix*.  Therefore PCA is really the eigenvalue decomposition of the covariance matrix!\n",
    "\n",
    "From this perspecive, the *eigenvalues* of $X^T X$ are the variances of the principal components.  The *eigenvectors* of $X^T X$ contain the weights (aka loadings) needed to tranform the original data into principal components.\n",
    "\n",
    "So by defining a matrix $W$ whose columns are the eigenvectors of $X^T X$ (arranged in order of decreasing eigenvalue), you can transform your original data $X$ into the *transformed* principal components by\n",
    "\n",
    "$$Z = XW.$$\n",
    "\n",
    "> This is what `sklearn`'s `PCA` does \"under the hood.\"  There is another module called `TruncatedSVD` that essentially does the same thing.  `TruncatedSVD`, however, can handle sparse matrices and plays a little nicer with memory, features that can be important in natural language processing!\n",
    "\n",
    "Now let's do this using `numpy`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "demo = pd.read_csv('./datasets/simple_demographics.csv')\n",
    "\n",
    "demo_noage = demo[['health','income','stress']]\n",
    "demo_noage = (demo_noage - demo_noage.mean()) / demo_noage.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by getting the correlation matrix, $X^T X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_noage_corr = np.corrcoef(demo_noage.values.T)\n",
    "demo_noage_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the eigenvalues and eigenvectors from the correlation matrix\n",
    "\n",
    "numpy has a convenient function to calculate this:\n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(demo_noage_corr)\n",
    "\n",
    "print(eig_vals)\n",
    "print(eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the explained variance\n",
    "\n",
    "A useful measure is the **explained variance**, which is calculated from the eigenvalues. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "### $$ ExpVar_i = \\bigg(\\frac{eigenvalue_i}{\\sum_j^n{eigenvalue_j}}\\bigg)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "\n",
    "var_exp = [(i / tot) for i in sorted(eig_vals, reverse=True)]\n",
    "print('Explained variance:            ', var_exp)\n",
    "\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print('Cumulative explained variance: ', cum_var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"projection\"></a>\n",
    "### Construct the Projection Matrix $W$\n",
    "\n",
    "This is simply a matrix of our top 2 eigenvectors (assuming we only want 2 principal components.)\n",
    "\n",
    "The eigenvectors are concatenated as columns.\n",
    "\n",
    "1. Start by ordering the eigenvectors by their corresponding eigenvalues biggest to smallest.\n",
    "- Concatenate the eigenvectors together. `np.stack()` is useful for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With numpy\n",
    "\n",
    "value_vector_pairs = [[eig_vals[i], eig_vecs[:,i]] for i in range(len(eig_vals))]\n",
    "value_vector_pairs.sort(reverse=True)\n",
    "value_vector_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vec_1 = value_vector_pairs[0][1]\n",
    "eig_vec_2 = value_vector_pairs[1][1]\n",
    "weight_2d_projection = np.stack((eig_vec_1, eig_vec_2), \n",
    "                                axis=1)\n",
    "\n",
    "print('Weight data 2d PCA projection matrix:\\n', weight_2d_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Transformed 2D Matrix $Z$\n",
    "\n",
    "To do this, we take the dot product of our 3D demographic matrix $X$ with the projection matrix $W$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = demo_noage.dot(weight_2d_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And now you know the **rest** of the story.  Good day!*  (Bonus points if you can correctly attribute this once-popular radio sign-off.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
