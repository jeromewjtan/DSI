{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Bayesian Inference\n",
    "\n",
    "> Author: Matt Brems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I want to use your biological parents' heights to predict your height as an adult.\n",
    "- $Y$: your height.\n",
    "- $X$: the average of your biological parents' heights.\n",
    "\n",
    "I want to fit a simple linear regression model $Y = \\beta_0 + \\beta_1X$. Let's try to infer reasonable values of $\\beta_1$.\n",
    "\n",
    "In frequentist inference, we let the data speak for itself.\n",
    "- If fitting a simple linear regression model $Y = \\beta_0 + \\beta_1X$, our \"prior knowledge\" about $\\beta_1$ is that $\\beta_1$ equally likely to be anywhere between $-\\infty$ and $+\\infty$. (In effect, there is **no prior knowledge**.)\n",
    "\n",
    "In Bayesian inference, we allow ourselves to include prior knowledge in the decision-making process.\n",
    "- If fitting a simple linear regression model $Y = \\beta_0 + \\beta_1X$, maybe we believe that $\\beta_1$ can reasonably only be between 0.8 and 1.2. We could select a Uniform(0.8, 1.2) prior or a Normal(1.0, 0.1) prior.\n",
    "\n",
    "### Why Bayes?\n",
    "- We can easily estimate $P(\\text{hypothesis}|\\text{data})$, which is what we want to learn about!\n",
    "- Just like scientific reasoning and research builds on previous research, **we can use our understanding of the world around us to better inform our inference** rather than to pretend like we need to start from scratch every time!\n",
    "- Every modeling choice we make is subjective! Selecting a prior isn't more or less subjective than removing a variable, adding an interaction term, imputing a certain value, etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how do I pick a prior distribution and a likelihood?\n",
    "\n",
    "Recall that a **distribution** is the set of all possible values of a variable and how frequently we observe each of the values.\n",
    "\n",
    "Let's say that I find a coin on the ground. We'll let our unknown probability of flipping heads be $p$.\n",
    "\n",
    "**Our goal, as will always be the case in Bayesian inference, is to find the posterior distribution of our unknown parameter. In this case, we want to find the posterior distribution of $p$, which is based on data we've observed and our prior beliefs.**\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{posterior} &\\propto& \\text{prior } \\times \\text{likelihood} \\\\\n",
    "&& \\\\\n",
    "f(p|\\text{data}) &\\propto& f(p) \\times f(\\text{data}|p)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "\n",
    "### Selecting a prior distribution\n",
    "\n",
    "Our posterior is a combination of our prior and our likelihood (data)!\n",
    "- If our prior beliefs are too strong, then our posterior will be \"dominated\" by our prior.\n",
    "- If our prior beliefs are too vague, then our posterior will be \"dominated\" by our data.\n",
    "\n",
    "For the rest of the lesson, we'll go through how to choose a prior distribution and a likelihood for $p$, then visualize the impacts on $p$.\n",
    "\n",
    "We want to construct a prior distribution for $p$. This prior distribution should summarize our beliefs about $p$ **before seeing any data**. If we know that $p$ must be between 0 and 1, then we should construct a prior distribution that reflects this knowledge.\n",
    "\n",
    "Suppose you and I have pretty strong beliefs that $p$, the probability of flipping heads, should be 0.5. It's possible that $p$ might be a bit less than 0.5 or a bit more than 0.5.\n",
    "- A very convenient distribution for this type of problem is called the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution). The Beta distribution is a continuous distribution bound between 0 and 1. Since probabilities can take on any value between 0 and 1, we often use the Beta distribution when trying to describe our beliefs about probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beta distribution is parameterized by $\\alpha$ and $\\beta$, much like the Normal distribution is parameterized by $\\mu$ and $\\sigma$.\n",
    "\n",
    "Let's plot a Beta distribution with parameters $\\alpha = 100$ and $\\beta = 100$, and look at it.\n",
    "- In code, we often use `a` and `b` as the arguments because `alpha` is such a common argument in functions and we don't want to confuse the Beta distribution with a `beta` argument for the Beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our alpha and beta parameters:\n",
    "a =\n",
    "b ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 x values in the appropriate\n",
    "# range to use for plotting:\n",
    "x ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate line plot.\n",
    "plt.plot(x, beta(a,b).pdf(x),\n",
    "         color='b', lw=2)\n",
    "\n",
    "# Plot customization:\n",
    "plt.ylabel('Density', fontsize = 14)\n",
    "plt.xlim((0,1))\n",
    "plt.xlabel('Values of p', fontsize = 14)\n",
    "plt.title(r'Beta($\\alpha=100$,$\\beta=100$)', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta(a, b):\n",
    "    # Generate 100 x values in the appropriate\n",
    "    # range to use for plotting:\n",
    "    x = np.linspace(0, 1, 100)\n",
    "\n",
    "    # Generate line plot.\n",
    "    plt.plot(x, beta(a,b).pdf(x),\n",
    "            'b', lw=2)\n",
    "\n",
    "    # Plot customization:\n",
    "    plt.ylabel('Density', fontsize = 14)\n",
    "    plt.xlim((0,1))\n",
    "    plt.xlabel(r'Values of $p$', fontsize = 14)\n",
    "    plt.title(fr'Beta($\\alpha={a}$,$\\beta={b}$)', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Beta distribution with alpha = 100, beta = 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take two minutes to explore the Beta distribution. Try to answer some of the following questions:\n",
    "- As `a` increases, what happens?\n",
    "- As `b` increases, what happens?\n",
    "- Under what circumstances will the Beta distribution be symmetric?\n",
    "- If I wanted a perfectly flat Beta distribution, what values of `a` and `b` would you select?\n",
    "- If I wanted a Beta distribution that was centered at exactly 50% and had very little variance (almost all of the density is on exactly 50%), what values of `a` and `b` would you select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>BONUS: The Beta distribution should be perfectly smooth. How can we get the visual to look smoother?</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "def plot_beta(a, b):\n",
    "    # Generate 1,000 x values in the appropriate\n",
    "    # range to use for plotting:\n",
    "    x = np.linspace(0, 1, 1_000)\n",
    "\n",
    "    # Generate line plot.\n",
    "    plt.plot(x, beta(a,b).pdf(x),\n",
    "            'b', lw=2)\n",
    "\n",
    "    # Plot customization:\n",
    "    plt.ylabel('Density', fontsize = 14)\n",
    "    plt.xlim((0,1))\n",
    "    plt.xlabel(r'Values of $p$', fontsize = 14)\n",
    "    plt.title(fr'Beta($\\alpha={a}$,$\\beta={b}$)', fontsize = 18);\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a likelihood\n",
    "\n",
    "Our posterior is a combination of our prior and our likelihood (data)!\n",
    "\n",
    "We want to construct a likelihood function for $\\text{data}|p$. We are saying, \"For every possible value of $p$, how likely is it that we observe this data?\"\n",
    "\n",
    "The likelihood function should model how the data are generated.\n",
    "\n",
    "<details><summary>If I flip a fixed number of coins, what distribution should I use to model the number of heads?</summary>\n",
    "A binomial distribution is probably the best model to use.\n",
    "</details>\n",
    "\n",
    "Suppose I flip my coin ten times and observe exactly 8 heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binomial(n_trials, k_successes):\n",
    "    # Generate 100 x values in the appropriate\n",
    "    # range to use for plotting:\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Plot binomial function assuming k_successes in n_trials.\n",
    "    y = []\n",
    "\n",
    "    # Generate line plot.\n",
    "    plt.plot()\n",
    "\n",
    "    # Plot customization:\n",
    "    plt.ylabel('Likelihood', fontsize = 14)\n",
    "    plt.xlim((0,1))\n",
    "    plt.xlabel(r'Values of $p$', fontsize = 14)\n",
    "    plt.title(fr'Binomial likelihood with $n={n_trials}$, $k={k_successes}$', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Binomial likelihood with 8 successes out of 10 trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that my likelihood isn't dependent on my prior. **My likelihood only depends on my data!**\n",
    "\n",
    "### The Binomial distribution is discrete. Why is my likelihood continuous??\n",
    "Our likelihood function is showing, \"For every possible value of $p$, how likely is it that we observe this data?\"\n",
    "\n",
    "When we talked about the Binomial distribution before, we imagined that the probability of success $p$ was fixed and known. We then looked at how likely it was to observe 8 heads given some fixed value of $p$.\n",
    "\n",
    "In Bayesian analysis, we've observed the data... now we're trying to learn about $p$! Given the data we've observed, how likely is it that $p$ takes on the value of 0, of 0.01, of 0.02, and so on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why do we observe a peak around 0.8 here?</summary>\n",
    "\n",
    "- Because flipped our coin ten times and saw eight heads! It makes sense that the likeliest value for our parameter $p$ is 80%, since we saw heads 80% of the time!\n",
    "- You might say that $p=0.8$ **maximizes our likelihood**... more on this in a later lesson!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take two minutes to explore the likelihood. Try to answer some of the following questions:\n",
    "- As `k`, your number of successes, increases, what happens?\n",
    "- As `n`, your number of trials, increases, what happens?\n",
    "- Under what circumstances will the likelihood be symmetric?\n",
    "- Suppose that instead of getting 7 heads out of 10 coins, you got 7000 heads out of 10000 flips. How does this influence your likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binomial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior * Likelihood: Simulating our Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta_binomial(a, b, n_trials, k_successes, steps = 100):\n",
    "    # Generate 100 x values in the appropriate\n",
    "    # range to use for plotting:\n",
    "    lst = np.linspace(0, 1, steps)\n",
    "    \n",
    "    # Calculating prior, likelihood, and posterior.\n",
    "    prior = [beta(a, b).pdf(i) for i in lst]\n",
    "    likelihood = [binom(n_trials, i).pmf(k = k_successes) for i in lst]\n",
    "    posterior = np.multiply(prior, likelihood)\n",
    "    \n",
    "    # Plotting colored lines here to show prior mode, the maximum likelihood value, and posterior mode.\n",
    "    plt.vlines([(a - 1) / (a + b - 2), k_successes / n_trials, (a + k_successes - 1) / (a + b + n_trials - 2)],\n",
    "               ymin = 0,\n",
    "               ymax = max(max(prior),max(likelihood), max(posterior)), # height of dotted lines\n",
    "               linestyles = 'dashed',\n",
    "               colors = ['tab:orange', 'tab:green', 'tab:blue'])\n",
    "    \n",
    "    # Plotting prior, likelihood, and posterior.\n",
    "    plt.title(\"Prior, Likelihood, and Posterior\")\n",
    "    plt.plot(lst, prior, c = 'tab:orange');\n",
    "    plt.plot(lst, likelihood, c = 'tab:green');\n",
    "    plt.plot(lst, posterior, c = 'tab:blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta_binomial(a = ,           # Hyperparameter alpha for Beta(alpha, beta)\n",
    "                   b = ,           # Hyperparameter beta for Beta(alpha, beta)\n",
    "                   n_trials = ,     # Hyperparameter n for Binomial(n, k)\n",
    "                   k_successes = )   # Hyperparameter k for Binomial(n, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with this function.\n",
    "- Pick the vaguest, flattest prior you can! How does this affect our posterior?\n",
    "- Pick a very, very specific prior that has almost all of its weight on 0.5. How does this affect your posterior?\n",
    "- Try three or four different values of `a` and `b` in your Beta distribution. How do those influence your prior? Your posterior?\n",
    "- Suppose that instead of getting 7 heads out of 10 coins, you got 7000 heads out of 10000 flips. How does this influence your likelihood? Your posterior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
