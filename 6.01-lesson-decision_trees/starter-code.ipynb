{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Decision Trees\n",
    "Authors: David Yerrington, Matt Brems\n",
    "\n",
    "<img src=\"./images/sms1.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand the intuition behind decision trees.\n",
    "- Calculate Gini.\n",
    "- Describe how decision trees use Gini to make decisions.\n",
    "- Fit, generate predictions from, and evaluate decision tree models.\n",
    "- Interpret and tune `max_depth`, `min_samples_split`, `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we get for dinner?\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|   Sushi   |      Sunny     |   Weekday  |\n",
    "|   Indian  |      Rainy     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a rainy day. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Indian food.\n",
    "- In 100% of past cases where the weather is rainy, we've eaten Indian food!\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "|   Indian  |      Rainy     |   Weekend  |\n",
    "|   Indian  |      Rainy     |   Weekday  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a sunny day. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Either Sushi or Mexican food... but we can't say with certainty whether we'd eat sushi or Mexican food.\n",
    "- Based on our past orders, we eat sushi on 1/3 of sunny days and we eat Mexican food on 2/3 of sunny days.\n",
    "- If I **had** to make a guess here, I'd probably predict Mexican food, but we may want to use additional information to be certain.\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Sushi   |      Sunny     |   Weekday  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a sunny day that also happens to be a weekend. Based on our past orders, what do you think we'll order?</summary>\n",
    "\n",
    "- Mexican food.\n",
    "- In 100% of past cases where the weather is sunny and where it's a weekend, we've eaten Mexican food!\n",
    "\n",
    "|$Y = $ Food|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "|  Mexican  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: Overview\n",
    "\n",
    "A decision tree:\n",
    "- takes a dataset consisting of $X$ and $Y$ data, \n",
    "- finds rules based on our $X$ data that partitions (splits) our data into smaller datasets such that\n",
    "- by the bottom of the tree, the values $Y$ in each \"leaf node\" are as \"pure\" as possible.\n",
    "\n",
    "We frequently see decision trees represented by a graph.\n",
    "\n",
    "<img src=\"./images/order_food_dt.png\" alt=\"order_food\" width=\"750\"/>\n",
    "\n",
    "- (This image was created using [Draw.io](https://www.draw.io/).)\n",
    "\n",
    "### Terminology\n",
    "Decision trees look like upside down trees. \n",
    "- What we see on top is known as the \"root node,\" through which all of our observations are passed.\n",
    "- At each internal split, our dataset is partitioned.\n",
    "- A \"parent\" node is split into two or more \"child\" nodes.\n",
    "- At each of the \"leaf nodes\" (colored orange), we contain a subset of records that are as pure as possible.\n",
    "    - In this food example, each leaf node is perfectly pure. Once we get to a leaf node, every observation in that leaf node has the exact same value of $Y$!\n",
    "    - There are ways to quantify the idea of \"purity\" here so that we can let our computer do most of the tree-building (model-fitting) process... we'll come back to this later.\n",
    "\n",
    "Decision trees are also called \"**Classification and Regression Trees**,\" sometimes abbreviated \"**CART**.\"\n",
    "- [DecisionTreeClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [DecisionTreeRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Questions\n",
    "\n",
    "If you aren't familiar with the game [20 Questions](https://en.wikipedia.org/wiki/Twenty_Questions), it's a game with two players (or teams). \n",
    "- Player A thinks of an item but doesn't say what the item is.\n",
    "- Player B then attempts to guess what the item is by asking a series of 20 questions with a yes or no answer.\n",
    "- If player B correctly guesses the item, then player B wins!\n",
    "- If player B does not correctly guess the item, then player A wins!\n",
    "\n",
    "Let's play a quick game of \"5 Questions\" to get a feel for it.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision trees operate in a fashion that's pretty similar to 20 Questions.\n",
    "- Decisions are made in a sequential fashion. Once you know a piece of information, you use that piece of information when asking future questions.\n",
    "    - Example: If you know that the item you're trying to guess is a person, then you can use that information to ask better subsequent questions.\n",
    "- It's possible to get lucky by making very specific guesses early, but it's pretty unlikely that this is a winning strategy.\n",
    "    - Example: If you asked, \"Is it an airplane? Is it a boat? Is it a car?\" as your first three questions, it's not very likely that you'll win the game.\n",
    "\n",
    "When fitting a decision tree, we're effectively getting our computer to play a game of 20 Questions. We give the computer some data and it figures out the best $X$ variable to split on at the right time.\n",
    "- Above, our \"what food should we order?\" decision tree first asked what the weather was, **then** asked whether it was a weekday or weekend.\n",
    "- If we had asked \"is it a weekday or weekend\" first, we'd have ended up with a slightly more complicated decision tree.\n",
    "\n",
    "Just like with all of our models, in order for the computer to learn which $X$ variable to split on and when, the computer needs a loss function to quantify how good a particular split is. This is where the idea of **purity** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity in Decision Trees\n",
    "\n",
    "When quantifying how \"pure\" a node is, we want to see what the distribution of $Y$ is in each node, then summarize this distribution with a number.\n",
    "\n",
    "<img src=\"./images/order_food_dt.png\" alt=\"order_food\" width=\"750\"/>\n",
    "\n",
    "- For continuous $Y$ (i.e. using a decision tree to predict income), the default option is mean squared error.\n",
    "    - This is the `criterion = 'mse'` argument in `DecisionTreeRegressor`.\n",
    "    - When the decision tree is figuring out which split to make at a given node, it picks the split that maximizes the drop in MSE from the parent node to the child node.\n",
    "    \n",
    "\n",
    "- For discrete $Y$, the default option is the Gini impurity. *(Bonus: This is not quite the same thing as the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient).)*\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "\\text{Gini impurity (2 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "\\text{Gini impurity (3 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our y variable from our \"where should we eat\" dataframe.\n",
    "y = ['Indian', 'Sushi', 'Indian', 'Mexican', 'Indian', 'Mexican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gini function, called gini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if your Gini function is correct on the \n",
    "# \"where should we eat\" data. (Should get 0.6111.)\n",
    "gini(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Practice\n",
    "\n",
    "<details><summary>What is the Gini impurity of a node when every item is from the same class?</summary>\n",
    "    \n",
    "- Our Gini impurity is zero.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 \\\\\n",
    "&=& 1 - 1^2 \\\\\n",
    "&=& 1 - 1 \\\\\n",
    "&=& 0\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when every item is from the same class?\n",
    "gini(['Indian', 'Indian', 'Indian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with two items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when we have two classes, each with two items?\n",
    "gini(['Indian', 'Indian', 'Mexican', 'Mexican'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with three items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when we have two classes, each with three items?\n",
    "gini(['Indian', 'Indian', 'Mexican', 'Mexican', 'Indian', 'Mexican'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have three classes, each with two items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.6667.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{9} - \\frac{1}{9} - \\frac{1}{9} \\\\\n",
    "&=& 1 - \\frac{1}{3} \\\\\n",
    "&=& \\frac{2}{3}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when we have three classes, each with two items?\n",
    "gini(['Indian', 'Indian', 'Mexican', 'Mexican', 'Sushi', 'Sushi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Summary of Gini Impurity Scores</summary>\n",
    "\n",
    "- In the binary case, Gini impurity ranges from 0 to 0.5.\n",
    "- If we have three classes, Gini impurity ranges from 0 to 0.66667.\n",
    "- If we have $k$ classes, Gini impurity ranges from 0 to $1-\\frac{1}{k}$.\n",
    "- In all cases, a Gini impurity of 0 means maximum purity - all of our observations are from the same class!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how does a decision tree use Gini to decide which variable to split on?\n",
    "\n",
    "- At any node, consider the subset of our dataframe that exists at that node.\n",
    "- Iterate through each variable that could potentially split the data.\n",
    "- Calculate the Gini impurity for every possible split.\n",
    "- Select the variable that causes the greatest decrease in Gini impurity from the parent node to the child node.\n",
    "\n",
    "One consequence of this is that a decision tree is fit using a **greedy** algorithm. Simply put, a decision tree makes the best short-term decision by optimizing at each node individually. _This might mean that our tree isn't optimal in the long run!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "import pandas as pd\n",
    "\n",
    "# Read in Titanic data.\n",
    "titanic = pd.read_csv('./titanic_clean.csv')\n",
    "\n",
    "# Change sex to float.\n",
    "titanic['Sex'] = titanic['Sex'].map({'male':0,\n",
    "                                     'female':1})\n",
    "\n",
    "# Create embarked_S column.\n",
    "titanic['Embarked_s'] = titanic['Embarked'].map({'S':1,\n",
    "                                                 'C':0,\n",
    "                                                 'Q':0})\n",
    "\n",
    "# Create embarked_C column.\n",
    "titanic['Embarked_c'] = titanic['Embarked'].map({'S':0,\n",
    "                                                 'C':1,\n",
    "                                                 'Q':0})\n",
    "\n",
    "# Conduct train/test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic.drop(['Survived','PassengerId','Name','Embarked'], axis=1),\n",
    "                                                    titanic['Survived'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_s</th>\n",
       "      <th>Embarked_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.6542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked_s  Embarked_c\n",
       "43        2    1   5.0      1      2  27.7500           1           0\n",
       "42        3    0  28.5      0      0   7.2292           0           1\n",
       "73        1    0  71.0      0      0  34.6542           0           1\n",
       "167       3    0  24.0      0      0   7.0500           1           0\n",
       "449       3    0  32.0      0      0   7.8542           1           0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out first five rows of X_train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model.\n",
    "dt ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.9939759036144579\n",
      "Score on testing set: 0.7009345794392523\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What conclusion would you make here?</summary>\n",
    "\n",
    "- Our model is **very** overfit to the data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting a decision tree, your model will always grow until it nearly perfectly predicts every observation!\n",
    "- This is like playing a game of 20 questions, but instead calling it \"Infinite Questions.\" You're always going to be able to win!\n",
    "\n",
    "<details><summary>Intuitively, what might you try to do to solve this problem?</summary>\n",
    "    \n",
    "- As with all models, try to gather more data.\n",
    "- As with all models, remove some features.\n",
    "- Is there a way for us to stop our model from growing? (Yes!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Decision Trees\n",
    "There are two hyperparameters of decision trees that we may commonly tune in order to prevent overfitting.\n",
    "\n",
    "- `max_depth`: The maximum depth of the tree.\n",
    "    - By default, the nodes are expanded until all leaves are pure (or some other argument limits the growth of the tree).\n",
    "    - In the 20 questions analogy, this is like \"How many questions we can ask?\"\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - By default, the minimum number of samples required to split is 2. That is, if there are two observations in a node and if we haven't already achieved maximum purity, we can split it!\n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node (a terminal node at the end of the tree).\n",
    "    - By default, the minimum nubmer of samples required in a leaf node is 1. (This should ring alarm bells - it's very possible that we'll overfit our model to the data!)\n",
    "\n",
    "[Source: Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with:\n",
    "# - a maximum depth of 5.\n",
    "# - at least 7 samples required in order to split an internal node.\n",
    "# - at least 3 samples in each leaf node.\n",
    "# - random state of 42.\n",
    "\n",
    "dt ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=7,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.8714859437751004\n",
      "Score on testing set: 0.7897196261682243\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model.\n",
    "print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's GridSearch to try to find the best tree.\n",
    "\n",
    "- Check [3, 5, 7, 10] for `max_depth`.\n",
    "- Check [5, 10, 15, 20] for `min_samples_split`.\n",
    "- Check [2, 3, 4, 5, 6, 7] for `min_samples_leaf`.\n",
    "- Run 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many models are being fit here?</summary>\n",
    "\n",
    "- 4 * 4 * 6 * 5 = 480 models.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0196499824523926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 480 out of 480 | elapsed:    3.0s finished\n",
      "/Users/mattbrems/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start our timer.\n",
    "\n",
    "\n",
    "# Let's GridSearch over the above parameters on our training data.\n",
    "\n",
    "\n",
    "# Stop our timer and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=6, min_samples_split=15,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is our best decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'min_samples_leaf': 6, 'min_samples_split': 15}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What were the best parameters of our decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8172690763052208"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the cross-validated score of the above decision tree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.8775100401606426\n",
      "Score on testing set: 0.7523364485981309\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with best parameters.\n",
    "\n",
    "\n",
    "# Fit model.\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model.\n",
    "print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set.\n",
    "preds ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101  21]\n",
      " [ 32  60]]\n"
     ]
    }
   ],
   "source": [
    "# Generate confusion matrix.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,\n",
    "                                  preds).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test,\n",
    "                       preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.6522\n"
     ]
    }
   ],
   "source": [
    "# Calculate sensitivity.\n",
    "\n",
    "sens =\n",
    "\n",
    "print(f'Sensitivity: {round(sens, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.8279\n"
     ]
    }
   ],
   "source": [
    "# Calculate specificity.\n",
    "\n",
    "spec =\n",
    "\n",
    "print(f'Specificity: {round(spec, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's so great about decision trees?\n",
    "\n",
    "\n",
    "### 1. We don't have to scale our data.\n",
    "Much in the same way that decision trees don't need much preprocessing in order to work, the base assumptions around how data is used and estimated, don't rely on scale to be effective.\n",
    "\n",
    "### 2. Decision trees don't care about how your data is distributed.\n",
    "\n",
    "Is your data heavily skewed or not normally distributed? Decision trees are nonparametric, meaning we don't make assumptions about how our data or errors are distributed.\n",
    "\n",
    "### 3. Easy to interpret.\n",
    "\n",
    "The output of a decision tree is easy to interpet and thus relatable to non-technical people. (We'll talk about `feature_importance` later.)\n",
    "\n",
    "### 4. Speed.\n",
    "\n",
    "Decision trees are fit very quickly!\n",
    "\n",
    "> **Protip**\n",
    ">\n",
    "> Consider creating a benchmark using a decision tree to understand how one might behave on its own before using a more complicated method.  Do your best to understand how a simple model behaves on a set of data, before using a more complex model. This is a great practice to get into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the downsides of decision trees?\n",
    "\n",
    "\n",
    "### 1. Decision trees can very easily overfit.\n",
    "Decision trees often suffer from high error due to variance, so we need to take special care to avoid this. (Luckily, there are lots of algorithms designed to do exactly this!)\n",
    "\n",
    "### 2. Decision trees are locally optimal.\n",
    "Because we're making the best decision at each node (greedy), we might end up with a worse solution in the long run.\n",
    "\n",
    "### 3. Decision trees don't work well with unbalanced data.\n",
    "We often will bias our results toward the majority class. We need to take steps to avoid this as well! (Check out the `class_weight` parameter if you're interested.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "0b28c6b3b13649718658d43e965c8062": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
