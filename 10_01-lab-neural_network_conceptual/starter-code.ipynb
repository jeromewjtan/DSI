{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lab\n",
    "\n",
    "> Author: Matt Brems (DC)\n",
    "\n",
    "In this lab, we are going to find out why neural networks are so good at what they do. Specifically, we'll do three things:\n",
    "1. Build and plot a \"complicated, wiggly\" function. This function could resemble any complicated real-world process that we would like to be able to model.\n",
    "2. Build a neural network from scratch that very closely approximates this function. (That is, the predicted values from the neural network are very, very close to the true values from the \"real\" function.)\n",
    "3. Understand the reason why neural networks are great, called the _Universal Approximation Theorem_.\n",
    "\n",
    "**Optional Video**: If you want a visual refresher on neural networks and their building blocks, head to [this link](https://www.youtube.com/watch?v=aircAruvnKk) and watch the 3Blue1Brown video on \"But what *is* a Neural Network?\" It's a **phenomenal** 20-minute introduction to solidify the building blocks of a neural network. (This is optional, but probably worth your time!)\n",
    "\n",
    "**Required Reading**: Head over to [this site](http://neuralnetworksanddeeplearning.com/chap4.html) and read from the beginning of the page until the \"Many Input Variables\" section.\n",
    "\n",
    "#### Recommended Order of Reading\n",
    "The introduction and the \"Two Caveats\" section are pretty mathematical in nature. I think this is a great resource, but wish that the author didn't start off with a mathematical theorem. If I were reading this page, I would:\n",
    "- skim the introduction,\n",
    "- skip the \"Two Caveats\" section,\n",
    "- read the \"Universality with One Input and One Output\" section,\n",
    "- go back and read the introduction,\n",
    "- then go back and read the \"Two Caveats\" section.\n",
    "\n",
    "(You do not need to read the \"Many Input Variables\" section and beyond but are certainly welcome to do so!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1:** Examine the image below. This shows a small neural network.\n",
    "\n",
    "<img src=\"./images/weight_bias.jpg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "In this image:\n",
    "- How many input nodes are there?\n",
    "- How many hidden nodes are there?\n",
    "- How many hidden layers are there?\n",
    "- How many output nodes are there?\n",
    "- What does $w$ mean? \n",
    "- While only one is explicitly drawn on the diagram, how many values of $w$ would we expect in this neural network?\n",
    "- What does $b$ mean? \n",
    "- While only one is explicitly drawn on the diagram, how many values of $b$ would we expect in this neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2:** For a sigmoidal activation function to closely resemble a step function, how would you describe the values of $w$ and $b$?\n",
    "\n",
    "_Hint:_ Try playing around with the applets on the page to test how different parts of the perceptron affect the output.\n",
    "\n",
    "_Note:_ In an attempt to make things easier, the article defines $s = \\frac{âˆ’b}{w}$. The author's goal is to only describe one parameter $s$ instead of describing $b$ and $w$. I find this notation confusing, because that notation only applies to this article and isn't used elsewhere. I'm going to stick with $b$ and $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3**: \n",
    "The author asks you to find values of $h_i$ that make your neural network closely approximate $f(x)$. Record your values of $h_i$ here and your best \"average deviation\" score.\n",
    "- Note $h$ is simply a weight. The author is calling it $h$ to correpond to height, but it's just a weight corresponding to the connection between the hidden layer and the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**: Build the neural network from your work in Problem 3 here.\n",
    "\n",
    "A few things to keep in mind:\n",
    "- How many inputs are there? \n",
    "- How many outputs are there?\n",
    "- How many neurons are in the hidden layer? \n",
    "- In order to create a step function at a specific value of $x$:\n",
    "    - what should the value of $w$ be?\n",
    "    - what should the value of $b$ be?\n",
    "    - use a [sigmoid activation function](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions).\n",
    "- What do the values of $h_i$ represent?\n",
    "- Use an [sigmoid activation function](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions) on the last layer.\n",
    "\n",
    "The values of $h$ on this diagram are **not correct**, but this image may be helpful to you as you plan out your network!\n",
    "\n",
    "<img src=\"./images/network.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sigmoid activation function.\n",
    "def sigmoid_activation():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your activation function so that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will take in your weight (w)\n",
    "# and where you want the step to occur, then solve for\n",
    "# the bias.\n",
    "\n",
    "def solve_for_bias():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model structure (topology) and define the weights and biases.\n",
    "\n",
    "\n",
    "# Specify weights and biases from input to hidden layer.\n",
    "steps =\n",
    "bias_hidden =\n",
    "weights_hidden =\n",
    "\n",
    "# Specify weights and biases from hidden to output layer.\n",
    "bias_output =\n",
    "\n",
    "# Remember: what are our values of weights from the hidden layer to the output layer?\n",
    "# These are the values of h that we got!\n",
    "weights_output = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run the network:\n",
    "def run_network():\n",
    "    \n",
    "    # Define your input value. This should just be one value.\n",
    "    input_value =\n",
    "    \n",
    "    # Calculate the value of the nodes in the hidden layer.\n",
    "    Z_hidden =\n",
    "    \n",
    "    # Use your activation function to transform the value of your nodes.\n",
    "    activation_hidden =\n",
    "\n",
    "    # Calculate the value of the nodes in the output layer.\n",
    "    Z_output =\n",
    "    \n",
    "    # Use your activation function to transform the value of your output node.\n",
    "    activation_output =\n",
    "    \n",
    "    # Return the output of your network!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**: Now that we've built our neural network, let's evaluate the performance of it!\n",
    "\n",
    "\n",
    "- Use `np.linspace` to generate 1000 values of $x$ between 0 and 1. \n",
    "- Your $y$ values are the actual observed values of $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ for each of the $x$ that you generated using `np.linspace`. Generate $y$ (using the true function $f(x))$ for all values of $x$.\n",
    "- Your $\\hat{y}$ in this case are your predicted values from your neural network for each of the $x$ that you generated using `np.linspace`.\n",
    "- Visually compare $y$ and $\\hat{y}$ by plotting both on one grid.\n",
    "- Compare $y$ and $\\hat{y}$ using mean squared error.\n",
    "\n",
    "Recall that mean squared error is given by:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}-y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values of X from 0 to 1.\n",
    "x_values =\n",
    "\n",
    "# Generate values of Y for each X from 0 to 1.\n",
    "def f(x):\n",
    "\n",
    "\n",
    "y =\n",
    "\n",
    "# Generate values of Y-hat for each X from 0 to 1.\n",
    "y_hat ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your predicted and observed values.\n",
    "plt.plot(x_values, y, label = 'True')\n",
    "plt.plot(x_values, y_hat, label = 'Predicted')\n",
    "plt.legend()\n",
    "plt.xlim((0,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This will vary depending on your values of $h$ - your MSE may differ from others' MSE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6**: Suppose you wanted to increase the performance of this neural network. How might you go about doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**: Summarize the Universal Approximation Theorem and show that it explains why neural networks can perform very well. (Don't copy it; use your own words!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
