{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Regularization\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Justin Pounders_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Define and describe regularization for regression models\n",
    "- Write the regularized loss function\n",
    "- Describe how regularization affects regression coefficients\n",
    "- Describe the differences between the Lasso, Ridge, and ElasticNet models\n",
    "- Implement and visualize the penalties using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Overview of regularization](#intro)\n",
    "- [What is the effect of regularization](#effect)\n",
    "- [Standardizing predictors is required](#standardization)\n",
    "- [Defining and visualizing the Ridge](#viz-ridge)\n",
    "- [Defining and visualizing the Lasso](#viz-lasso)\n",
    "- [Defining and visualizing the Elastic-Net](#viz-elastic-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Overview of regularization\n",
    "\n",
    "---\n",
    "\n",
    "### Why regularization?\n",
    "\n",
    "Before we get to regularization we need to review **overfitting**:\n",
    "\n",
    "> *Overfitting* means building a model that matches the training data \"too closely.\" The model ends up training on noise rather than signal.\n",
    "\n",
    "- Usually caused by a model that is too complex \n",
    "- Low bias/high variance models\n",
    "\n",
    "---\n",
    "\n",
    "**Do you need to worry about overfi!ing with Linear Regression?**\n",
    "\n",
    "\"Good\" properties\n",
    "- Low complexity\n",
    "- High bias/low variance \n",
    "- Does not tend to overfit\n",
    "\n",
    "Danger zone\n",
    "- Including irrelevant features (signal v noise)\n",
    "- (number of features) is close to (number of observations)\n",
    "- Correlated inputs\n",
    "- Numerically large coefficients\n",
    "\n",
    "---\n",
    "\n",
    "### What is regularization\n",
    "\n",
    "**The goal of \"regularizing\" regression models is to structurally prevent overfitting by imposing a penalty on the coefficients of the model.**\n",
    "\n",
    "Regularization methods like the Ridge and Lasso add this additional \"penalty\" on the size of coefficients to the loss function. When the loss function is minimized, this additional component is added to the residual sum of squares.\n",
    "\n",
    "> In other words, model training becomes a balance between error and the size of the coefficients. \n",
    "\n",
    "The two most common types of regularization are the **Lasso**, **Ridge**. There is a mixture of them called the **Elastic Net**. We will take a look at the mathematics of regularization and the effect these penalties have on model fits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lsq-loss'></a>\n",
    "\n",
    "## Review: Least Squares Loss Function\n",
    "\n",
    "---\n",
    "\n",
    "Ordinary least squares regression minimizes the mean squared error (MES) to fit the data:\n",
    "\n",
    "### $$ \\text{minimize:}\\; MSE(\\beta_0, \\beta_1, ...) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "Where our model predictions for $y$ are based on the sum of the $\\beta_0$ intercept and the products of $\\beta_i$ with $x_i$.\n",
    "\n",
    "What should I take away from this?\n",
    "\n",
    "- Machine learning usually boils down to _optimization_\n",
    "- Training/fitting means _minimizing_ a \"loss function\" (aka cost or objective function)\n",
    "- In linear regression, the loss function is MSE, and the optimization is performed with respect to the $\\beta$ coefficients.\n",
    "\n",
    "> Minimizing the loss function gives you the optimal $\\beta$ values for your regression model!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='effect'></a>\n",
    "\n",
    "## What is the effect of regularization?\n",
    "\n",
    "---\n",
    "\n",
    "**To demonstrate the effects we will be using a dataset on wine quality.**\n",
    "\n",
    "An important aspect of this data, which is a reason why we might choose to use regularization, is that there is **multicollinearity** in the data. The term multicollinearity means that there are _high correlations between predictor variables in your model_. \n",
    "\n",
    "**This can lead to a variety of problems including:**\n",
    "1. The effect of predictor variables estimated by your regression will depend on what other variabes are included in your model.\n",
    "2. Predictors can have wildly different effects depending on the observations in your sample, and small changes in samples can result in very different estimated effects.\n",
    "3. With very high multicollinearity, the inverse matrix the computer calculates may not be accurate.\n",
    "4. We can no longer interpret a coefficient on a variable as the effect on the target of a one unit increase in that variable holding the other variables constant. This is because when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n",
    "\n",
    "> **For example:** say I want to predict home prices.  As predictors I have number of bedrooms upstairs, number of bedrooms downstairs and total number of bedrooms.  What could go wrong?!?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the wine csv\n",
    "\n",
    "This version has red and white wines concatenated together and tagged with a binary 1,0 indicator (1 is red wine). There are many other variables purportedly related to the rated quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('./datasets/winequality_merged.csv')\n",
    "\n",
    "# replace spaces in column names and convert all columns to lowercase:\n",
    "wine.columns = [x.lower().replace(' ','_') for x in wine.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the correlation between variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_corr = wine.corr()\n",
    "wine_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default matplotlib figure size to 7x7:\n",
    "fix, ax = plt.subplots(figsize=(9,7))\n",
    "\n",
    "# Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
    "mask = np.zeros_like(wine_corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Plot the heatmap with seaborn.\n",
    "# Assign the matplotlib axis the function returns. This will let us resize the labels.\n",
    "ax = sns.heatmap(wine_corr, mask=mask, ax=ax)\n",
    "\n",
    "# Resize the labels.\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14)\n",
    "\n",
    "# If you put plt.show() at the bottom, it prevents those useless printouts from matplotlib.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardization'></a>\n",
    "\n",
    "## Standardizing predictors is required\n",
    "\n",
    "---\n",
    "\n",
    "With the Lasso and Ridge it is neccessary to standardize the predictor columns before constructing the models, even the dummy coded categorical variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a target/dependent variable that we will predict\n",
    "target = 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the columns that are not the target\n",
    "nc = [x for x in wine.columns if x != target]\n",
    "\n",
    "# By subtracting the mean and dividing by the standard devation, the normalization procedure is putting \n",
    "# all of the predictor variables on the same scale (distributions with mean == 0 and standard deviation == 1)\n",
    "# (Yes, we could have also used sklearn's StandardScaler.  Try it and see if you get the same answer! )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the target and predictor matrix \n",
    "\n",
    "We will be making two X, Y datasets. One of them will be dramatically more complex (and thus overfit).\n",
    "\n",
    "1. The first is going to just be all the variables added together.\n",
    "2. The second will have all interactions between the variables included. This should overfit the target quite a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With patsy (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "# Get the non-target cols with a simple list comprehension\n",
    "non_target_cols = [c for c in wine.columns if c != target]\n",
    "print('Non-target columns are...')\n",
    "print(non_target_cols)\n",
    "print()\n",
    "\n",
    "# Use some string adding and joining to make the simple model formula:\n",
    "formula_simple = target + ' ~ ' + ' + '.join(non_target_cols) + ' -1'\n",
    "print('The \"simple\" patsy formula is...')\n",
    "print(formula_simple)\n",
    "print()\n",
    "\n",
    "# Make the complex formula:\n",
    "formula_complex = target + ' ~ (' + ' + '.join(non_target_cols) + ')**2 -1'\n",
    "print('The \"complex\" patsy formula is...')\n",
    "print(formula_complex)\n",
    "print()\n",
    "\n",
    "# Create the X and Y pairs for both!\n",
    "Y, X = patsy.dmatrices(formula_simple, data=wine, return_type='dataframe')\n",
    "Yoverfit, Xoverfit = patsy.dmatrices(formula_complex, data=wine, return_type='dataframe')\n",
    "\n",
    "Y = Y.values.ravel()\n",
    "Yoverfit = Yoverfit.values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at how the shapes of the X matrices differ. You can see that the Xoverfit has considerably more columns due to the creation of all possible two-way interactions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-ridge'></a>\n",
    "\n",
    "## Defining and Visualizing the Ridge\n",
    "\n",
    "---\n",
    "\n",
    "### $$ \\text{minimize:}\\; MSE+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$\\beta_j^2$ is the squared coefficient for variable $x_j$.\n",
    "\n",
    "$\\sum_{j=1}^n \\beta_j^2$ is the sum of these squared coefficients for every variable in the model. This does **not** include the intercept $\\beta_0$.\n",
    "\n",
    "$\\alpha$ is a constant for the _strength_ of the regularization parameter. The higher the value, the greater the impact of this new component in the loss function. If the value was zero, we would revert back to just the least squares loss function. If the value was a billion, however, the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term.\n",
    "\n",
    "Now all **you** have to do is...\n",
    "\n",
    "import the `Ridge` model class from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates over a series of different alpha regularization parameters and stores the results of the model so that we can plot them interactively.\n",
    "\n",
    "> This will not be part of your typical workflow.  This for learning purposes only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_coefs(X, Y, alphas):\n",
    "    \n",
    "    # set up the list to hold the different sets of coefficients:\n",
    "    coefs = []\n",
    "    \n",
    "    # Set up a ridge regression object\n",
    "    ridge_reg = Ridge()\n",
    "    \n",
    "    # Iterate through the alphas fed into the function:\n",
    "    for a in alphas:\n",
    "        \n",
    "        # On each alpha reset the ridge model's alpha to the current one:\n",
    "        ridge_reg.set_params(alpha=a)\n",
    "        \n",
    "        # fit or refit the model on the provided X, Y\n",
    "        ridge_reg.fit(X, Y)\n",
    "        \n",
    "        # Get out the coefficient list\n",
    "        coefs.append(ridge_reg.coef_)\n",
    "        \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha values for the ridge are best visualized on a logarithmic \"magnitude\" scale. Essentially, the effect of alpha on the coefficients does not increase linearly but by orders of magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.logspace gives us points between specified orders of magnitude on a logarithmic scale. It is base 10.\n",
    "r_alphas = np.logspace(0, 5, 200)\n",
    "\n",
    "# Get the coefficients for each alpha for the Ridge, using the function above\n",
    "r_coefs = ridge_coefs(X, Y, r_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotting function below will:\n",
    "\n",
    " - Plot the effect of changing alpha on the coefficient size on a **path** graph\n",
    " - Plot the effect of changing alpha on the coefficient size on a **bar** graph\n",
    " \n",
    "Each one gives informative information. It's just two different ways of visualizing the same thing. The chart is interactive so you can play around with the values of alpha across the specified range above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cycler package lets us \"cycle\" throug colors.\n",
    "# Just another thing i had to look up on stackoverflow. That's my life.\n",
    "from cycler import cycler\n",
    "\n",
    "def coef_plotter(alphas, coefs, feature_names, to_alpha, regtype='ridge'):\n",
    "    \n",
    "    # Get the full range of alphas before subsetting to keep the plots from \n",
    "    # resetting axes each time. (We use these values to set static axes later).\n",
    "    amin = np.min(alphas)\n",
    "    amax = np.max(alphas)\n",
    "    \n",
    "    # Subset the alphas and coefficients to just the ones below the set limit\n",
    "    # from the interactive widget:\n",
    "    alphas = [a for a in alphas if a <= to_alpha]\n",
    "    coefs = coefs[0:len(alphas)]\n",
    "    \n",
    "    # Get some colors from seaborn:\n",
    "    colors = sns.color_palette(\"husl\", len(coefs[0]))\n",
    "    \n",
    "    # Get the figure and reset the size to be wider:\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18,5)\n",
    "\n",
    "    # We have two axes this time on our figure. \n",
    "    # The fig.add_subplot adds axes to our figure. The number inside stands for:\n",
    "    #[figure_rows|figure_cols|position_of_current_axes]\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    \n",
    "    # Give it the color cycler:\n",
    "    ax1.set_prop_cycle(cycler('color', colors))\n",
    "    \n",
    "    # Print a vertical line showing our current alpha threshold:\n",
    "    ax1.axvline(to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    \n",
    "    # Plot the lines of the alphas on x-axis and coefficients on y-axis\n",
    "    ax1.plot(alphas, coefs, lw=2)\n",
    "    \n",
    "    # set labels for axes:\n",
    "    ax1.set_xlabel('alpha', fontsize=20)\n",
    "    ax1.set_ylabel('coefficients', fontsize=20)\n",
    "    \n",
    "    # If this is for the ridge, set this to a log scale on the x-axis:\n",
    "    if regtype == 'ridge':\n",
    "        ax1.set_xscale('log')\n",
    "    \n",
    "    # Enforce the axis limits:\n",
    "    ax1.set_xlim([amin, amax])\n",
    "    \n",
    "    # Put a title on the axis\n",
    "    ax1.set_title(regtype+' coef paths\\n', fontsize=20)\n",
    "    \n",
    "    # Get the ymin and ymax for this axis to enforce it to be the same on the \n",
    "    # second chart:\n",
    "    ymin, ymax = ax1.get_ylim()\n",
    "\n",
    "    # Add our second axes for the barplot in position 2:\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # Position the bars according to their index from the feature names variable:\n",
    "    ax2.bar(list(range(1, len(feature_names)+1)), coefs[-1], align='center', color=colors)\n",
    "    ax2.set_xticks(list(range(1, len(feature_names)+1)))\n",
    "    \n",
    "    # Reset the ticks from numbers to acutally be the names:\n",
    "    ax2.set_xticklabels(feature_names, rotation=65, fontsize=12)\n",
    "    \n",
    "    # enforce limits and add titles, labels\n",
    "    ax2.set_ylim([ymin, ymax])\n",
    "    ax2.set_title(regtype+' predictor coefs\\n', fontsize=20)\n",
    "    ax2.set_xlabel('coefficients', fontsize=20)\n",
    "    ax2.set_ylabel('alpha', fontsize=20)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ipython widgets so we can make this plotting function interactive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function and `interact` from ipywidgets lets me take some specified alphas that we have already calculated the coefficients for and plot them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_plot_runner(log_of_alpha=0):\n",
    "    coef_plotter(r_alphas, r_coefs, X.columns, 10**log_of_alpha, regtype='ridge')\n",
    "\n",
    "interact(ridge_plot_runner, log_of_alpha=(0.000001,5.0,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-lasso'></a>\n",
    "\n",
    "## Defining and Visualizing the Lasso\n",
    "\n",
    "---\n",
    "\n",
    "Now we do the same thing as above but for the Lasso. You will be able to see how the coefficients change differently for both!  But first, let's define lasso.\n",
    "\n",
    "Lasso regression takes a different approach. Instead of adding the sum of squared $\\beta$ coefficients to the RSS, it adds the sum of the absolute values of the $\\beta$ coefficients:\n",
    "\n",
    "### $$ \\text{minimize:}\\; MSE + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$|\\beta_j|$ is the absolute value of the $\\beta$ coefficient for variable $x_j$.\n",
    "\n",
    "$\\alpha$ is again the strength of the regularization penalty component in the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as the ridge coefficient by alpha calculator\n",
    "def lasso_coefs(X, Y, alphas):\n",
    "    coefs = []\n",
    "    lasso_reg = Lasso()\n",
    "    for a in alphas:\n",
    "        lasso_reg.set_params(alpha=a)\n",
    "        lasso_reg.fit(X, Y)\n",
    "        coefs.append(lasso_reg.coef_)\n",
    "        \n",
    "    return coefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alphas for the Lasso tend to effect regularization linearly rather than by orders of magnitude like in the ridge. \n",
    "\n",
    "A linear series of alphas is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_alphas = np.arange(0.001, 0.15, 0.0025)\n",
    "l_coefs = lasso_coefs(X, Y, l_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same plotting function above, but now with the calculated coefficients of alpha for the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_plot_runner(alpha=0):\n",
    "    coef_plotter(l_alphas, l_coefs, X.columns, alpha, regtype='lasso')\n",
    "\n",
    "interact(lasso_plot_runner, alpha=(0.001,0.2,0.0025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-elastic-net'></a>\n",
    "\n",
    "## Visualizing the Elastic Net\n",
    "\n",
    "---\n",
    "\n",
    "Can't decide?  Buy both!!\n",
    "\n",
    "The Elastic Net combines the Ridge and Lasso penalties.  It adds *both* penalties to the loss function:\n",
    "\n",
    "> \"[Elastic Net] allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\"\n",
    "-- sklearn docs\n",
    "\n",
    "### $$ \\text{minimize:}\\; MSE + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\rho\\sum_{j=1}^p |\\beta_j| + \\alpha(1-\\rho)\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "In the elastic net, the effect of the ridge versus the lasso is balanced by the $\\rho$ parameter.  It is the ratio of Lasso penalty to Ridge penalty and must be between zero and one.\n",
    "\n",
    "`ElasticNet` in sklearn has two parameters:\n",
    "- `alpha`: the regularization strength.\n",
    "- `l1_ratio`: the amount of L1 vs L2 penalty (i.e., $\\rho$). An l1_ratio of 0 is equivalent to the Ridge, whereas an l1_ratio of 1 is equivalent to the Lasso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coefficients with both alpha values and an l1_ratio of 0.05. Lasso can \"overpower\" the Ridge penalty in some datasets, and so rather than an equal balance I'm just adding a little bit of Lasso in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as the ridge coefficient by alpha calculator\n",
    "def elasticnet_coefs(X, Y, alphas):\n",
    "    coefs = []\n",
    "    enet_reg = ElasticNet()\n",
    "    for a in alphas:\n",
    "        enet_reg.set_params(alpha=a, l1_ratio=0.05)\n",
    "        enet_reg.fit(X, Y)\n",
    "        coefs.append(enet_reg.coef_)\n",
    "        \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_alphas = np.arange(0.01, 1.0, 0.005)\n",
    "enet_coefs = elasticnet_coefs(X, Y, enet_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enet_plot_runner(alpha=0.01):\n",
    "    coef_plotter(enet_alphas, enet_coefs, X.columns, alpha, regtype='elastic net')\n",
    "\n",
    "interact(enet_plot_runner, alpha=(0.0,1.0,0.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-performance'></a>\n",
    "\n",
    "## Model performance of the Ridge, Lasso and Elastic Net on the overfit data\n",
    "\n",
    "### (i.e., what do I actually need to do!)\n",
    "\n",
    "---\n",
    "\n",
    "Let's check out how the penalties affect the performance of our models. On the basic wine dataset there won't be much benefit to using regularization. We can switch over to the overfit data instead to see if regularization helps us control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start with plain vanilla linear regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's make it into a Ridge regression by adding the penalty term:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which $\\alpha$ to chose?  Let's search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what is the optimal value of alpha?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's average the CV scores for each value of alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A picture is worth a thousand words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have seen how MSE is minimized at the optimal $\\alpha$.  Let's get the corresponding value of $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-aways\n",
    "\n",
    "---\n",
    "\n",
    "- **Ridge** is good at \"shrinking\" model coefficients.\n",
    "- **Lasso** is good at eliminating coefficients.\n",
    "- **ElasticNet** combines Ridge and Lasso.\n",
    "\n",
    "In all cases, \"regularization strength\" is defined by a parameter $\\alpha$ (sometimes called $\\lambda$):\n",
    "- Increase $\\alpha$ (turn up regularization) \n",
    "    - Increase bias\n",
    "    - Decrease variance\n",
    "- Decrease $\\alpha$ (turn down regularization) \n",
    "    - Decrease bias\n",
    "    - Increase variance\n",
    "    \n",
    "Bottom line? \n",
    "\n",
    "- The Ridge is best suited to deal with multicollinearity. \n",
    "- Lasso also deals with multicollinearity between variables, but in a more brutal way (it \"zeroes out\" the less effective variable).\n",
    "- The Lasso is particularly useful when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you by forcing coefficients to be zero.\n",
    "- Elastic Net combines both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "03860d2d80204ca295d01e93e8e99474": {
     "views": [
      {
       "cell_index": 41
      }
     ]
    },
    "b535fb165fa343b297ba42fb4a55c6fa": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    },
    "f5d5ef714eee4c61b085a3bb6b96cd73": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "fdc5e91596ea49fa84bf4aed6d37b849": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
