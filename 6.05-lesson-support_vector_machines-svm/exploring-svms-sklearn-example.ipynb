{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Codealong Exploring SVMs Using Sklearn\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Justin Pounders_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Describe how the SVM builds its decision threshold.\n",
    "- Explain the concept of the maximum margin hyperplane.\n",
    "- Describe the bias-variance tradeoff with respect to SVMs (i.e., how the regularization constant C allows SVMs to fit non-linearly separable problems.)\n",
    "- Use the kernel trick to transform problems from non-linearly separable to linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, linear_model, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SVM\n",
    "\n",
    "Support Vector Machines (SVMs) are *statistical models* used for **classification**.\n",
    "\n",
    "---\n",
    "\n",
    "Review:\n",
    "- What is classification?\n",
    "- What other classification models have you seen?\n",
    "\n",
    "---\n",
    "\n",
    "### How to Think about SVM\n",
    "\n",
    "- The *geometric intuition* of SVMs is easier to grasp than the\n",
    "- *mathematical constructs* needed to make it work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Separability\n",
    "\n",
    "SVMs work really well for data in which the classes are *linearly separable*.\n",
    "\n",
    "\n",
    "![fit](./assets/linear_separability_vs_not.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum-Margin Estimator\n",
    "\n",
    "If classes are linearly seperable, SVM finds the *hyperplane* that sepearates the classess with *maximum margin*.\n",
    "\n",
    "![fit](./assets/Margin.png)\n",
    "\n",
    "**Why maximize the margin?**\n",
    "\n",
    "- SVM solves for a decision boundary that should minimize the generalization error.\n",
    "- Observations near the decision boundary are the most \"ambiguous\"\n",
    "- SVM defines it's fit using the most ambiguous points\n",
    "\n",
    "![fit](./assets/linear_sep_support_vecs_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if data are not linearly seperable?\n",
    "\n",
    "- Still want to minimize $\\|w\\|$ (maximize margin)\n",
    "- Would also like to minimize a *loss function* that penalizes points for being on the \"wrong side\"\n",
    "\n",
    "We can use the *hinge loss function*:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{hinge loss} &= \\sum_{i=1}^n \\max\\left[ 0, 1-y_i(w^Tx_i+b)\\right] \\\\\n",
    "&= \\begin{cases}\n",
    "0 & \\text{if } x \\text{ outside or on margin} \\\\ > 0 & \\text{if } x \\text{ within margin}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Hinge loss penalizes misclassified points!*\n",
    "\n",
    "Put \"simply,\" we want to minimize\n",
    "\n",
    "$$ C \\times \\left[ \\text{hinge loss} \\right] + \\left[ \\frac{1}{\\text{margin width}} \\right]$$\n",
    "\n",
    "where $C$ is a hyperparameter.  We can rewrite this slightly to see it as\n",
    "\n",
    "$$ \\left[ \\text{hinge loss} \\right] + \\frac{1}{C} \\left[ \\frac{1}{\\text{margin width}} \\right]$$\n",
    "\n",
    "$$\\sum_{i=1}^N \\max\\left(0, 1 - y_i(w^T x_i + b)\\right) +  \\frac{1}{C} ||w||^2$$\n",
    "\n",
    "i.e., loss + 1/C * regularization (c.f. Ridge!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Takeaway*: **Bias/variance trade-off** is handled via the hyperparameter $C$\n",
    ">\n",
    "> - Large $C \\rightarrow$ narrow margin, less tolerant of misclassification, tends toward high variance\n",
    "> - Small $C \\rightarrow$ wider margin, more tolerant of misclassification, tends toward high bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "import imp\n",
    "plotter = imp.load_source('plotter', './code/svm_plotter.py')\n",
    "from plotter import SVMPlotter\n",
    "\n",
    "svm_plotter = SVMPlotter()\n",
    "svm_plotter.initialize_data(class_n=20)\n",
    "svm_plotter.svm_interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if your data are not separable?\n",
    "\n",
    "Like, no where *close* to linearly separable?  **We can use something called the \"kernel trick.\"\n",
    "\n",
    "> Data that is not linearly sepearable in its default space may become linearly seperable in a higher dimensiosnal space.\n",
    "\n",
    "\n",
    "![fit](./assets/kernel_trick.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9NrALgHFwTo\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel choices** (in `sklearn`):\n",
    "\n",
    "- Linear\n",
    "- Polynomial\n",
    "- Gaussian, a.k.a. Radial Basis Functions (RBF)\n",
    "\n",
    "![fit](./assets/nonlinear-1.png)\n",
    "\n",
    "![fit](./assets/nonlinear-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Digit Classification\n",
    "\n",
    "### 1. Load the handwritten digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "digits = datasets.load_digits()\n",
    "digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-validate a logistic regression on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cross-validate a SVM on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian SVM has two parameters, gamma and C\n",
    "\n",
    "---\n",
    "\n",
    "### gamma\n",
    "\n",
    "Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. \n",
    "\n",
    "The higher the value of gamma, the more it will try to exactly fit the training data set. Will cause over-fitting problem.\n",
    "- small gamma: the model is constrained, can under-fit!  high bias and low variance.\n",
    "- big gamma: Tries to capture the shape too well: can over-fit!  low bias and high variance.\n",
    "\n",
    "\n",
    "### C\n",
    "\n",
    "Penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly. C can be thought of as the parameter for the soft margin cost function, which controls the influence of each individual support vector\n",
    "\n",
    "- small C: makes the decision surface smooth and simple, softer margin can under-fit! high bias and low variance.\n",
    "- big C: selects more support vectors: can over-fit! harder margin. low bias and high variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit an SVM modifying the default gamma and C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gridsearch an optimal gamma with C=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. [Bonus... because it's slow!] Gridsearch the optimal C, gamma, and kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "gamma_range = np.logspace(-5, 2, 10)\n",
    "C_range = np.logspace(-3, 2, 10)\n",
    "kernel_range = ['rbf', 'sigmoid', 'linear', 'poly']\n",
    "\n",
    "param_grid = dict(gamma=gamma_range, C=C_range, kernel=kernel_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pros and Cons of SVMs\n",
    "\n",
    "*Pros*\n",
    "\n",
    "- Exceptional perfomance (historically widely used)\n",
    "- Robust to outliers\n",
    "- Effective in high dimensional data\n",
    "- Can work with non-linearities\n",
    "- Fast to compute even on non-linear (kernel trick)\n",
    "- Low risk of overfitting\n",
    "\n",
    "*Cons*\n",
    "\n",
    "- Blackbox\n",
    "- Can be slow on large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use SVM vs. Logistic Regression\n",
    "\n",
    "*(Advice from Andrew Ng)*\n",
    "\n",
    "- **If there are more feature than training samples:**\n",
    "    Use logistic regression or SVM without a kernel (\"linear kernel\")\n",
    "- **If there are about 10 times as many samples as features:**\n",
    "    Use SVM with a Gaussian kernel  \n",
    "- **If there are many more training samples than features:**\n",
    "    Spend time feature engineering, then use logistic regression or SVM without a kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [For a really great resource check out these slides (some of which are cannabalized in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "- [This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n",
    "- SVM docs on [SKLearn](http://scikit-learn.org/stable/modules/svm.html)\n",
    "- Iris example on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#example-svm-plot-iris-py)\n",
    "- Hyperplane walkthrough on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py)\n",
    "- A comprehensive [user guide](http://pyml.sourceforge.net/doc/howto.pdf) to SVM. My fav!\n",
    "- A [blog post tutorial](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/) of understanding the linear algebra behind SVM hyperplanes. Check [part 3](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/) of this blog on finding the optimal hyperplane\n",
    "- This [Quora discussion](https://www.quora.com/How-do-you-teach-Support-Vector-Machine-to-a-beginner-in-Machine-Learning) includes a high-level overview plus a [20min video](https://www.youtube.com/watch?v=aDbsJ_S3tIA) walking through the core \"need-to-knows\"\n",
    "- A [slideshow introduction](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf) to the optimization considerations of SVM\n",
    "- A second [slideshow overview from UCF](http://www.cs.ucf.edu/courses/cap6412/fall2009/papers/Berwick2003.pdf) on the highnotes of SVM\n",
    "- Andrew Ng's [notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) on SVM from CS 229\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=eHsErlPJWUU) (1hr+) from one of my fav lecturers (Dr Yasser) on SVM. He does a followup on [kernel tricks](https://www.youtube.com/watch?v=XUj5JbQihlU) too\n",
    "- A [FULL LECTURE](https://www.youtube.com/watch?v=_PwhiWxHK8o) (50min) (from MIT Opencoursewar)\n",
    "- An infamous [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf) (cited 7000+ times!) on why SVM is a great text classifier\n",
    "- An [advanced discussion](http://www.icml-2011.org/papers/386_icmlpaper.pdf) of SVMs as probabilistic models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. [Extra Practice] Import the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data[:, :2]  # we only take the first two features. \n",
    "iris_y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. [Extra Practice] Cross-validate a default logistic regression and default SVM on the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. [Extra Practice] Compare three SVMs with different kernels on the iris data visually.\n",
    "- Gaussian\n",
    "- Linear\n",
    "- Poly of degree 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. [Bonus] Compare SVM kernels visually on fake data using sklearn's `make_circles`.\n",
    "\n",
    "Load `make_circles` from here:\n",
    "```python\n",
    "from sklearn.datasets import make_circles\n",
    "```\n",
    "\n",
    "Compare the linear, rbf, and poly kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "circles_X, circles_y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "plt.scatter(circles_X[:,0], circles_X[:,1], c=circles_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate linear, polynomial (degree 3) and rbf kernels with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
