{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Clustering and K-Means\n",
    "\n",
    "_Authors: Tim Book (DC), Dave Yerrington (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/kYWumd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Understand basic unsupervised clustering problems\n",
    "- Perform a K-Means Clustering Analysis\n",
    "- Evaluate clusters for fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson Guide\n",
    "- [Unsupervised learning](#unsupervised)\n",
    "- [Introduction to clustering](#intro)\n",
    "- [Uses for Clustering](#uses)\n",
    "- [What is clustering?](#what)\n",
    "- [KNN review](#knn)\n",
    "- [Clustering algorithms](#algos)\n",
    "- [K-means clustering](#k-means)\n",
    "- [K-Means step-by-step](#km-steps)\n",
    "- [K-Means: a visual example](#vis)\n",
    "- [K-Means caveats and pitfalls](#caveats)\n",
    "- [Centroid Initialization](#init-centroid)\n",
    "- [Choosing K](#choose-k)\n",
    "- [A note on K-Means convergence](#converge)\n",
    "- [K-Means in sklearn](#sklearn)\n",
    "- [Metrics: inertia and the silhouette coefficient](#sil)\n",
    "- [Practice: use K-Means on the \"Isotopic Composition Plutonium Batches\" data](#pluto)\n",
    "    - [How does scaling affect fit?](#scaling)\n",
    "- [Conclusion: K-Means tradeoffs](#conclusion)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='unsupervised'></a>\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "---\n",
    "\n",
    "Up until now, we've been doing **supervised learning** - that is, modeling of the form:\n",
    "\n",
    "> Given X, predict Y\n",
    "\n",
    "Supervised learning comes in two flavors: can you remind me what they are? And how are they different? (prompt)\n",
    "\n",
    "When we don't have a Y variable to predict, we are in the realm of **unsupervised learning**. Since there is no Y variable, unsupervised learning has no measurable \"goal\". Instead, unsuprvised learning seeks to **represent the data in new ways**. Today we're introducing **clustering**, however there are many other types of unsupervised learning.\n",
    "\n",
    "> Data that don't have a natural Y variable are sometimes referred to as **unlabeled** data. This is because the Y variable is often refered to as a **label**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Issues\n",
    "Since there is no Y variable to \"supervise\" our learning, unsupervised learning presents us with some new issues we've never had to work through:\n",
    "\n",
    "* **What is \"correct\"?** Since there's no Y variable, we don't have an easy to way know if we're even doing a good job.\n",
    "* **Tuning parameter selection.** Many unsupervised models have tuning parameters. How do we tune them if we don't know how to evaluate our model?\n",
    "* **Unpredictability (clustering).** In clustering, it is very difficult to predict what our model will give us. It's possible that a clustering algorithm won't give us actionable results. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "# Introduction to Clustering\n",
    "\n",
    "---\n",
    "\n",
    "**Clustering** is a task in which we seek to group our observations in **homogenous clusters**. Since it's unsupervised, it is up to us, the data scientists, to decide what we mean by \"homogenous\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"uses\"></a>\n",
    "## Uses for Clustering\n",
    "### Marketing\n",
    "Marketing teams do a lot of data-driven research into who does and does not buy their product (and why). As a marketing data scientist, you might collect demographic information about people in a survey and their spending habits. After clustering, you do some EDA and you might discover:\n",
    "\n",
    "> People in Cluster A aren't buying our product, but people in Cluster B are. Why?\n",
    "\n",
    "After some digging, you might make the conclusion and recommendation:\n",
    "\n",
    "> People in Cluster A have characteristic X, but people in Cluster B do not. In order to sell to Cluster A, we should target our marketing with respect to X.\n",
    "\n",
    "Maybe X = \n",
    "* They don't have cable\n",
    "* Their political beliefs\n",
    "* They live in cities\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/BdfATE.jpg\" style=\"width: 500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political Polling\n",
    "In the same vein as the above example, instead of buying a product, maybe it's **voting for a certain candidate**.\n",
    "\n",
    "The popular data-drive journalism website, FiveThirtyEight is doing a lot of research into the upcoming 2020 Democratic primary season. They have used (probably subjective) clustering techniques to divide Democratic voters into five clusters:\n",
    "\n",
    "* Party Loyalists\n",
    "* The Left\n",
    "* Millennials and Friends\n",
    "* Black voters\n",
    "* Hispanic voters (sometimes in combination with Asian voters)\n",
    "\n",
    "They then graded each candidate based on their favorability of these clusters:\n",
    "\n",
    "<img src=\"assets/imgs/five-corners.png\">\n",
    "\n",
    "Their methodology is probably more subjective than scientific, but it still provides a good example into how real political polling works. You can read more about it [here](https://fivethirtyeight.com/features/the-5-key-constituencies-of-the-2020-democratic-primary/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "Online retailers cluster their items by similarity. If you buy (or search for) a few items in a given cluster, they may recommend you other similar items in that same cluster.\n",
    "\n",
    "<img src=\"assets/imgs/recs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://snag.gy/YUt5RO.jpg\" style=\"float: left; margin-right: 25px; width: 250px\">\n",
    "\n",
    "## What other problems do you think can make use of clustering?\n",
    "\n",
    "(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"algos\"></a>\n",
    "# Clustering algorithms \n",
    "\n",
    "---\n",
    "\n",
    "The are many different algorithms that can perform clustering given a dataset:\n",
    "\n",
    "- **K-Means** (mean centroids)\n",
    "- **Hierarchical** (nested clusters by merging or splitting successively)\n",
    "- **DBSCAN** (density based)\n",
    "- **Affinity Propagation** (graph based approach to let points 'vote' on their preferred 'exemplar')\n",
    "- **Mean Shift** (can find number of clusters)\n",
    "- **Spectral Clustering**\n",
    "- **Agglomerative Clustering** (suite of algorithms all based on applying the same criteria/characteristics of one cluster to others)\n",
    "\n",
    "\n",
    "\n",
    "Today we're going to look only at one of the algorithms: **k-means**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside about the letter k:\n",
    "This is the _third_ topic we're learning that uses $k$ as its primary tuning parameter: k-Nearest Neighbors, k-Fold Cross Validation, and now k-Means Clustering. **The only thing they have in common is the letter $k$!** And since it's a tuning parameter, it's not the same $k$. Just an unfortunate coincidence that they all picked the same arbitrary letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='k-means'></a>\n",
    "# K-Means clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### K-Means is the most popular clustering algorithm\n",
    "\n",
    "K-means is one of the easier methods to understand and other clustering techniques use some of the same assumptions that k-means relies on.\n",
    "\n",
    "- **k** is the number of clusters.\n",
    "- **Means** refers to the mean points of the k clusters.\n",
    "\n",
    "The goal is to partition the data into set such that the total sum of squared distances from each point to the mean point of the cluster is minimized.\n",
    "\n",
    "**You must choose $k$, the number of clusters, in advance.** Note that this is a huge issue, since we rarely have an intuition for this number. And since we're in unsupervised territory, it's hard for us to know if we've picked correctly! We'll talk about how to pick $k$ later.\n",
    "\n",
    "The algorithm takes your entire dataset and iterates over its features and observations to determine clusters based around center points. These center points are known as **centroids**. \n",
    "\n",
    "**K-means iterative fitting:**\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize $k$ 'centroids' (starting points) in your data\n",
    "3. Create your clusters. Assign each point to the nearest centroid. \n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "> Check out a demo of this algorithm [here](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='caveats'></a>\n",
    "## Sounds great - what could go wrong?\n",
    "\n",
    "---\n",
    "\n",
    "Now that we've learned the basic process for K-Means clustering, we should explore the idea that nothing's perfect!  Let's play a game called **\"How will it converge!?\"**.\n",
    "\n",
    "### Scenario 1: Wispy Flat Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, color = make_blobs(n_samples=50, random_state=13, centers=2, cluster_std=.5, )\n",
    "X = map(lambda x: (x[0][0], x[0][1], color[x[1]]), zip(X, color))\n",
    "\n",
    "points = pd.DataFrame(list(X), columns=[\"x1\", \"x2\", \"color\"])\n",
    "ax = points.plot(kind=\"scatter\", x=\"x1\", y=\"x2\", figsize=(8, 4))\n",
    "\n",
    "initial_centroids = pd.DataFrame([[5.5, 2.4, \"b\"], [6.7, 2.4, \"r\"]], columns=[\"x1\", \"x2\", \"color\"])\n",
    "initial_centroids.plot(kind=\"scatter\", x=\"x1\", y=\"x2\", c=initial_centroids['color'], ax=ax, marker=\"*\", s=150);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2:  The Lone Wolf Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, color = make_blobs(n_samples=50, random_state=20, centers=2, cluster_std=.5, )\n",
    "X = list(map(lambda x: (x[0][0], x[0][1], color[x[1]]), zip(X, color)))\n",
    "\n",
    "X.append([34, 6, 0])\n",
    "\n",
    "points = pd.DataFrame(list(X), columns=[\"x1\", \"x2\", \"color\"])\n",
    "ax = points.plot(kind=\"scatter\", x=\"x1\", y=\"x2\", figsize=(8, 4))\n",
    "plt.text(35, 5.9, '\"Woof\"')\n",
    "ax.set_ylim(top=15, bottom=0)\n",
    "ax.set_xlim(left=-5, right=40)\n",
    "\n",
    "initial_centroids = pd.DataFrame([[2, 10, \"b\"], [15, 6, \"r\"]], columns=[\"x1\", \"x2\", \"color\"])\n",
    "initial_centroids.plot(kind=\"scatter\", x=\"x1\", y=\"x2\", c=initial_centroids['color'], ax=ax, marker=\"*\", s=150);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:  Data vs Centroid Initialization\n",
    "\n",
    "1. How much is data responsible for \"good\" clusters?\n",
    "1. Can you think of any ways to mitigate any potential problems with K-Means based on what you know so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many K?\n",
    "\n",
    "Sometimes it's obvious, sometimes it's not!  What do you think?\n",
    "\n",
    "- /poll \"How many K:  Figure 1\" \"1\" \"2\" \"3\" \"4\"\n",
    "- /poll \"How many K:  Figure 2\" \"1\" \"2\" \"3\" \"4\"\n",
    "- /poll \"How many K:  Figure 3\" \"1\" \"2\" \"3\" \"4\"\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/4rU39.png\"><br>1</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/gq28F.png\"><br>2</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"https://snag.gy/cWPgno.jpg\"><br>3</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"init-centroid\"></a>\n",
    "## Initializing Centroids\n",
    "\n",
    "---\n",
    "\n",
    "There are different methods of initializing centroids. For instance:\n",
    "\n",
    "- Randomly\n",
    "- Manually\n",
    "- Special KMeans++ method in Sklearn (_This initializes the centroids to be generally distant from each other_)\n",
    "\n",
    "**Depending on your problem, you may find some of these are better than others.**\n",
    "\n",
    "> **Note:** Manual is recommended if you know your data well enough to see the clusters without much help, but rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='choose-k'></a>\n",
    "## Choosing $k$\n",
    "This still remains an open question. Afterall we're tuning a tuning parameter with no metric for success! Here are three ideas:\n",
    "\n",
    "* Make an educated guess\n",
    "    - Industry knowledge (there are five kinds of Democrats...)\n",
    "    - Visualization (probably impossible if you have more than 2 variables)\n",
    "* Judge based on a pseudo-evaluation metric, like the **silhouette score**.\n",
    "* If you're using the resulting cluster labels as input to a supervised learning method later, you can tune $k$ to have the best supervised learning model. This is **transfer learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='converge'></a>\n",
    "## A note on K-Means convergence\n",
    "\n",
    "---\n",
    "\n",
    "In general, k-means will converge to a solution and return a partition of k clusters, even if no natural clusters exist in the data.  It's entirely possible – in fact, *common* – that the clusters do not mean anything at all. \n",
    "\n",
    "**Knowing your domain and dataset is essential. Evaluating the clusters visually is a must (if possible).**\n",
    "\n",
    "> _\"Given enough time, k-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='kmeans++' parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.\"_ [sklearn Clustering Guide](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "![](http://www.datamilk.com/kmeans_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"sklearn\"></a>\n",
    "## K-Means in sklearn\n",
    "\n",
    "---\n",
    "\n",
    "Below we will implement K-Means using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Let's make some more blobs to test K-Means on\n",
    "X, _ = make_blobs(n_samples=100, random_state=29, centers=3, cluster_std=1.5)\n",
    "X = map(lambda x: (x[0][0], x[0][1]), zip(X, color))\n",
    "\n",
    "df = pd.DataFrame(list(X), columns=[\"x1\", \"x2\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the scatter\n",
    "ax = df.plot(kind=\"scatter\", x=\"x1\", y=\"x2\", figsize=(12,6), s=50)\n",
    "plt.xlabel(\"$X_1$\", fontsize=18)\n",
    "plt.ylabel(\"$X_2$\", fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare our X matrix\n",
    "# Don't forget to scale! Why do we scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# (thread) How many K here?\n",
    "# I do: Fit a k-means clustering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After we fit our data, we can get our predicted labels from `model.labels_` and the center points`model.cluster_centers_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Check out centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Class attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# attach predicted cluster to original points\n",
    "df['cluster'] = km.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do: Show me the mean values for x1 and x2 WITHIN EACH CLUSTER for our data. (thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visually verifying cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe for cluster_centers (centroids)\n",
    "centroids = pd.DataFrame(\n",
    "    sc.inverse_transform(km.cluster_centers_),\n",
    "    columns=[\"x1\", \"x2\"]\n",
    ")\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "## Plot scatter by cluster / color, and centroids\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "df['color'] = df['cluster'].map(lambda p: colors[p])\n",
    "\n",
    "ax = df.plot(    \n",
    "    kind=\"scatter\", \n",
    "    x=\"x1\", y=\"x2\",\n",
    "    figsize=(10,8),\n",
    "    c = df['color']\n",
    ")\n",
    "\n",
    "centroids.plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"x1\", y=\"x2\", \n",
    "    marker=\"*\", c=[\"r\", \"g\", \"b\"], s=550,\n",
    "    ax=ax\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='sil'></a>\n",
    "\n",
    "## Metrics: inertia and the silhouette score\n",
    "\n",
    "---\n",
    "\n",
    "**Inertia** -- sum of squared errors for each cluster.\n",
    "- low inertia = dense cluster\n",
    "\n",
    "> As the number of clusters k increases, inertia decreases. \n",
    "\n",
    "**Silhouette Score** \n",
    "\n",
    "- measure of how far apart clusters are\n",
    "- high Silhouette = clusters are well separated\n",
    "\n",
    "The Silhouette Score is the averge of Silhouette $s$ of each point within a cluster in relation to other clusters. The logic behind the solhouette score is that **cohesion** should be low, while **separation** should be high:\n",
    "\n",
    "**cohesion** = Average distance of points within clusters\n",
    "\n",
    "**separation** = Average distance of points in one cluster to points in other clusters\n",
    "\n",
    "And it's calculated as:\n",
    "\n",
    "$$s_i = \\frac{b_i - a_i}{max\\{a_i, b_i\\}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $a_i$ = Average distance from point $x_i$ to all other points **in the same cluster**.\n",
    "* $b_i$ = Average distance from point $x_i$ to all points **in the next nearest cluster**.\n",
    "\n",
    "And the resulting silhouette score is the average of all the $s_i$s.\n",
    "\n",
    "* -1 = Bad\n",
    "\n",
    "* 0 = Meh\n",
    "\n",
    "* 1 = Good\n",
    "\n",
    "The silhouette score evaluates the viability of your clusters. It is a way to quantify the \"cohesion\" of local points to their clusters and the separation to other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy to calculate Silhouette in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programatically Inspecting $k$\n",
    "\n",
    "Since we're familliar with gridsearch now, let's try it out!  Gridsearch is typically used for supervised learning with some scoring objective for optimizing parameters in terms of `best_model`.  We don't really care about the best model as much as we want to see how either inertia or silhouette score changes based on $k$.\n",
    "\n",
    "We will write a basic process for exploring inertia and silhouette, that you can feel free to adapt to any other ML process you might write in Sklearn for doing custom scoring (using GridsearchCV is better for supervised learning because it has cross validation built-in!).\n",
    "\n",
    "> Visually inspecting your clusters is vastly superior to relying on programatic parameter searching for an ideal cluster for most data.  You can also invent your own evalutation metrics other than inertia or silhoette.  The world is also bigger than k-means when it comes to cluster applications.  It's a great first stop in your journey of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(2, 31):\n",
    "    cl = KMeans(n_clusters=k)\n",
    "    cl.fit(X_sc)\n",
    "    inertia = cl.inertia_\n",
    "    sil = silhouette_score(X_sc, cl.labels_)\n",
    "    scores.append([k, inertia, sil])\n",
    "    \n",
    "score_df = pd.DataFrame(scores)\n",
    "score_df.columns = ['k', 'inertia', 'silhouette']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "axes[0].plot(score_df.k, score_df.inertia)\n",
    "axes[0].set_title('Inertia over k')\n",
    "axes[1].plot(score_df.k, score_df.silhouette);\n",
    "axes[1].set_title('Silhouette Score over k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion: K-Means tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "**K-Means:**\n",
    "- Unsupervised clustering model\n",
    "- Iteratively finds labels given k\n",
    "- Easy to implement in sklearn\n",
    "- Sensitive to shape, scale of data **(!!!)**\n",
    "- Optimal k hard to evaluate\n",
    "\n",
    "---\n",
    "\n",
    "| Strengths | Weaknesses |\n",
    "| -- | -- |\n",
    "| k-Means is popular because it's simple and computationally efficient. | However, k-Means is highly scale dependent and isn't suitable for data of varying shapes and densities. |\n",
    "| Easy to see results / intuitive. | Evaluating results is more subjective, requiring much more human evaluation than trusted metrics. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-Means is sensitive to outliers\n",
    "\n",
    "![](https://snag.gy/WFNMQY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-Means is sensitive to centroid initialization\n",
    "\n",
    "![](https://snag.gy/5sigCD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- Andrew Moore's CS class at Carnegie Mellon contains good static visualization, step-by-step. His slide deck is online here: https://www.autonlab.org/tutorials/kmeans11.pdf. He also links to more of his tutorials on the first page. \n",
    "- [Sci-Kit Learn Clustering Overview](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "- [SKLearn K-Means Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "- [SKLearn Clustering Code - See _k_init__ for explanation of k++ centroid selection](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/k_means_.py#L769)\n",
    "- [Clustering Tutorial](http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/)\n",
    "- [Wikipedia's Deep Dive on Clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "- [An excellent blog from Galvanize on K-Means Clustering](http://www.galvanize.com/blog/introduction-k-means-cluster-analysis/#.V9aXqpOAOkp)\n",
    "\n",
    "**Some helpful stackexchange questions:**\n",
    "- http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust\n",
    "- http://stats.stackexchange.com/questions/174556/k-means-clustering-with-dummy-variables\n",
    "- http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\n",
    "- http://www-01.ibm.com/support/docview.wss?uid=swg21477401\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
