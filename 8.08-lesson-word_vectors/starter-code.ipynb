{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Word Vectors\n",
    "\n",
    "> Creator: Matt Brems (DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/upgrade Gensim.\n",
    "!pip install gensim --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gensim.\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer.\n",
    "t0 = time.time()\n",
    "\n",
    "# Import word vectors into \"model.\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('change_filepath')\n",
    "\n",
    "# Print results of timer.\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a vector?\n",
    "\n",
    "There are lots of ways to think about a vector.\n",
    "\n",
    "![](./images/vector.png)\n",
    "\n",
    "In **physics**, vectors are arrows.\n",
    "\n",
    "![](./images/vector.jpg)\n",
    "\n",
    "In **computer science** and **statistics**, vectors are columns of values, like one numeric Series in a DataFrame.\n",
    "\n",
    "#### It turns out that these are equivalent.\n",
    "\n",
    "![](./images/vector_on_graph.png)\n",
    "\n",
    "[This video](https://www.youtube.com/watch?v=fNk_zzaMoSs) does an exceptional job explaining vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what is a word vector?\n",
    "\n",
    "A word vector, simply, is a way for us to represent words with vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How have we technically already done this?</summary>\n",
    "    \n",
    "- CountVectorizer and TFIDFVectorizer. By representing each word as a new column in our DataFrame, we have represented words with vectors.\n",
    "\n",
    "![](./images/countvectorizer.jpeg)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more precise, we can think of each word as its own dimension or axis. In the example below, we have represented the horizontal axis with a vector for `cat` and the vertical axis with a vecvtor for `hat`.\n",
    "\n",
    "![](./images/cat_hat.png)\n",
    "\n",
    "This is exactly what CountVectorization and TFIDFVectorization have done; we are now just representing it geometrically/visually! Each column in our DataFrame corresponds to a new axis.\n",
    "\n",
    "## A little math: the dot product\n",
    "\n",
    "One thing we have spent lots of time talking about is whether or not two things are dependent or independent.\n",
    "- We assume that our $Y$ variable depends on the $X$ variables in our models.\n",
    "- We assume that our $X$ variables are independent of one another in linear models.\n",
    "- We frequently assume that our observations are independent of one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Thus far, how have we detected if two columns/vectors are dependent or independent?</summary>\n",
    "    \n",
    "- The most common way for us to detect for dependence/independence is **correlation**. \n",
    "    - If the correlation between two columns is far from zero, we say the two are dependent.\n",
    "    - If the correlation between two columns is close to zero, we say that the two are (linearly) independent.    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically, we say two vectors are independent if they are [orthogonal](https://en.wikipedia.org/wiki/Orthogonality) (perpendicular) to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Are the cat and hat vectors independent of one another?</summary>\n",
    "    \n",
    "- Yes! \n",
    "    - They are orthogonal to one another.\n",
    "    - They are perpendicular to one another. \n",
    "    - They form right angles. \n",
    "    - The three preceding bullet points are all equivalent: if one of them is true, then they will all be true and if one of them is false, then they will all be false.    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we detect this mathematically? **The dot product.**\n",
    "\n",
    "The [dot product of two vectors](https://en.wikipedia.org/wiki/Dot_product) $\\mathbf{a} = [a_1, a_2, \\ldots, a_p]$ and $\\mathbf{b} = [b_1, b_2, \\ldots, b_p]$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{a} \\cdot \\mathbf{b} &=& \\sum_{i=1}^p a_i \\times b_i \\\\\n",
    "&=& (a_1 \\times b_1) + (a_2 \\times b_2) + \\cdots + (a_p \\times b_p) \\\\\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Calculate the dot product of the vectors cat and hat. What is the value?</summary>\n",
    "    \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{cat} \\cdot \\mathbf{hat} &=& \\sum_{i=1}^2 cat_i \\times hat_i \\\\\n",
    "&=& (cat_1 \\times hat_1) + (cat_2 \\times hat_2) \\\\\n",
    "&=& (1 \\times 0) + (0 \\times 1) \\\\\n",
    "&=& 0\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- The dot product of the vectors `cat` and `hat` is 0.\n",
    "- [We could have also written](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.dot.html)\n",
    "```python\n",
    "np.dot([1,0],[0,1])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dot product using Python.\n",
    "np.dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the dot product between two vectors is zero, that means the vectors are (linearly) independent of one another!\n",
    "\n",
    "### When we say \"geometrically independent\" or \"mathematically independent\" or \"statistically linearly independent,\" these all mean the same thing! We just use different ways to detect them. (For example, if I have 100 vectors, I probably can't visually look at them and conclude independence. I probably have to use the dot product to determine independence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of vectorization of words (turning each word into its own column) is known as \"1-of-N encoding.\"\n",
    "\n",
    "![](./images/word2vec-one-hot.png)\n",
    "\n",
    "For example:\n",
    "- the vector for the word `king` would be [1, 0, 0, 0, 0].\n",
    "- the vector for the word `queen` would be [0, 1, 0, 0, 0].\n",
    "- the vector for the word `man` would be [0, 0, 1, 0, 0].\n",
    "- the vector for the word `woman` would be [0, 0, 0, 1, 0].\n",
    "- the vector for the word `child` would be [0, 0, 0, 0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Which of pairs of the above words are independent of one another?</summary>\n",
    "    \n",
    "- They all are! If you calculate the dot product for any of these pairs of words, you will get a value of zero. \n",
    "- When vectorizing words in this way, we treat words as independent of one another.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dot product using Python.\n",
    "np.dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking purely about language and the way we use it, **should** king and queen be independent of one another? **Should** man and woman be independent of one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do you think?</summary>\n",
    "    \n",
    "- Probably not!\n",
    "- King and queen have similar meanings. (Really, only the sex is different.)\n",
    "- Man and woman have similar meanings. (i.e. I know that \"man\" and \"woman\" are more similar than \"man\" and \"book\" or \"woman\" and \"car.\")\n",
    "- Our current data science strategy for NLP (CountVectorization, TFIDFVectorization) is good in that it allows us to get computers to understand natural language in a way similar to how humans do... but our current strategy has its limitations!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than creating a whole new dimension each time we encounter a new word and treating it as independent of all other words, can we instead come up with \"new axes\" that allow us to better understand meanings and relationships among words?\n",
    "- YES.\n",
    "\n",
    "**Word embedding** is a term used to describe representing words in mathematical space.\n",
    "- One word embedding technique is CountVectorization.\n",
    "- A more advanced word embedding technique is `Word2Vec`.\n",
    "\n",
    "### Word2Vec\n",
    "- Word2Vec is an approach that takes in observations (sentences, tweets, books) and maps them into some other space using a neural network.\n",
    "\n",
    "![](./images/word2vec-one-hot.png)\n",
    "\n",
    "In this example, you can \"think\" of a five-dimensional space. \n",
    "- The horizontal axis corresponds to `king`.\n",
    "- The vertical axis corresponds to `queen`.\n",
    "- The axis extending out toward you corresponds to `man`.\n",
    "- Given that we live in 3D space, we can't really visualize higher dimensions.\n",
    "\n",
    "Instead of giving each word its own axis, the `Word2Vec` algorithm will take all of our words and map them to another set of axes that accounts for these relationships.\n",
    "\n",
    "![](./images/word2vec-king-queen-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How can I tell that these vectors are not independent just by looking at them?</summary>\n",
    "    \n",
    "- They are not perpendicular to each other!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care?\n",
    "The structure of language has a lot of valuable information in it! The way we organize our text/speech tells us a lot about what things mean.\n",
    "\n",
    "By using machine learning to \"learn\" about the structure and content of language, our models can now organize concepts and learn the relationships among them.\n",
    "- Above, we did not explicitly tell the computer what \"king\" or \"queen\" or \"man\" or \"woman\" actually mean. But by learning from the data, our model can quantify the relationship among these entities!\n",
    "\n",
    "![](./images/word2vec-king-queen-composition.png)\n",
    "\n",
    "If we represent words with vectors, then we can define \"distances\" among words and do operations on them!\n",
    "- For example, if I take the \"king\" vector and subtract the \"man\" vector, what's leftover might be the idea of \"royalty.\"\n",
    "- If I take \"royalty\" and add \"woman\" to it, then I get \"queen!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Word2Vec work?\n",
    "\n",
    "#### Basic Answer:\n",
    "The idea is that we can use the position of words in sentences (i.e. see which words were commonly used together) to understand their relationships.\n",
    "- If \"king\" and \"queen\" are used near one another a lot, then it suggests that there may be some sort of relationship between them.\n",
    "- If \"king\" and \"queen\" are used near similar words a lot (i.e. \"throne,\" \"royal,\" \"princess,\" \"prince,\" \"heir\"), then it suggests that there may be some sort of relationship between them.\n",
    "\n",
    "#### More Advanced Answer:\n",
    "There are two algorithms that use neural networks to learn these relationships: Continuous Bag-of-Words (CBOW) and Continuous Skip-grams.\n",
    "\n",
    "![](./images/cbow.png)\n",
    "\n",
    "**CBOW (BONUS)**\n",
    "\n",
    "A continuous Bag-of-Words model is a two-layer neural network that:\n",
    "- takes the surrounding \"context words\" as an input.\n",
    "- generates the \"focus word\" as the output.\n",
    "![](./images/word2vec-cbow.png)\n",
    "\n",
    "**Skip-Gram (BONUS)**\n",
    "\n",
    "A Continuous Skip-gram model is a two-layer neural network that:\n",
    "- takes the \"focus word\" as an input.\n",
    "- generates the surrounding \"context words\" as the output.\n",
    "\n",
    "![](./images/skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neat application 1: Which of these is not like the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(['man', 'woman', 'king', 'queen', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own and share the most mind-blowing one in a thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application of this**: Suppose you're attempting to automatically detect spam emails or detect plagiarism based on words that don't belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neat application 2: What is most alike?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application of this**: Suppose you're building out a process to detect when people are tweeting about an emergency. They may not just use the word \"emergency.\" Rather than manually creating a list of words people could use, you may want to learn from a much larger corpus of data than just your personal experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neat application 3: Analogies\n",
    "\n",
    "In physics, we can add/subtract vectors to understand how two forces might act on an object. With word vectors, we can do the same thing!\n",
    "\n",
    "![](./images/word2vec-king-queen-composition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{king - man} &=& \\text{queen - woman} \\\\\n",
    "x_1 - x_2 &=& y_1 - y_2 \\\\\n",
    "x_1 - x_2 - y_1 &=& -y_2 \\\\\n",
    "-x_1 + x_2 + y_1 &=& y_2 \\\\\n",
    "\\text{king is to man as queen is to...} &&\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analogy function.\n",
    "\n",
    "    \n",
    "    # Find the vector y2 that is closest to $x_1 - x_2 + y_2$.\n",
    "\n",
    "    \n",
    "    # Return the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the images in this lesson were pulled from [this amazing resource](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word vectors from your own corpus! (BONUS)\n",
    "\n",
    "### NOTE: This will usually take a *long* time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# If you want to use gensim's data, import their downloader\n",
    "# and load it.\n",
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "\n",
    "# If you have your own iterable corpus of cleaned data:\n",
    "\n",
    "# Train a model! \n",
    "model = Word2Vec(corpus,      # Corpus of data.\n",
    "                 size=100,    # How many dimensions do you want in your word vector?\n",
    "                 window=5,    # How many \"context words\" do you want?\n",
    "                 min_count=1, # Ignores words below this threshold.\n",
    "                 sg=0,        # SG = 1 uses SkipGram, SG = 0 uses CBOW (default).\n",
    "                 workers=4)   # Number of \"worker threads\" to use (parallelizes process).\n",
    "\n",
    "# Do what you'd like to do with your dataa!\n",
    "model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the documentation for Gensim's implementation of [Word2Vec here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
